-- phpMyAdmin SQL Dump
-- version 4.2.7.1
-- http://www.phpmyadmin.net
--
-- Host: 127.0.0.1
-- Generation Time: Jul 17, 2016 at 04:08 PM
-- Server version: 5.6.20
-- PHP Version: 5.5.15

SET SQL_MODE = "NO_AUTO_VALUE_ON_ZERO";
SET time_zone = "+00:00";


/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!40101 SET NAMES utf8 */;

--
-- Database: `penelitian_yulianti_oenang`
--

-- --------------------------------------------------------

--
-- Table structure for table `data_penelitian`
--

CREATE TABLE IF NOT EXISTS `data_penelitian` (
`id` int(11) NOT NULL,
  `judul` text NOT NULL,
  `peneliti` text NOT NULL,
  `tahun_publikasi` int(11) NOT NULL,
  `masalah` text NOT NULL,
  `deskripsi_masalah` text NOT NULL,
  `keyword` text NOT NULL,
  `domain_data` text NOT NULL,
  `deskripsi_domain_data` text NOT NULL,
  `metode` text NOT NULL,
  `deskripsi_metode` text NOT NULL,
  `hasil` text NOT NULL,
  `creater` text NOT NULL
) ENGINE=InnoDB  DEFAULT CHARSET=latin1 AUTO_INCREMENT=176 ;

--
-- Dumping data for table `data_penelitian`
--

INSERT INTO `data_penelitian` (`id`, `judul`, `peneliti`, `tahun_publikasi`, `masalah`, `deskripsi_masalah`, `keyword`, `domain_data`, `deskripsi_domain_data`, `metode`, `deskripsi_metode`, `hasil`, `creater`) VALUES
(1, 'Supporting Multilingual Information Retrieval in Web Applications: An English-Chinese Web Portal Experiment', 'Jialun Qin, Yilu Zhou, Michael Chau, Hsinchun Chen', 2003, 'Evaluating a multilingual web', 'Developing and evaluating a multilingual English-Chinese Web portal in the business domain with CLIR and MLIR.', 'Multilingual Information Retrieval', 'The AI Lab SpidersRUs toolkit', 'The AI Lab SpidersRUs toolkit is used to build the English and Chinese collections for the Web portal. Each collection consists of about 100,000 IT business Web pages.<br/>\r\nWe built our own lexicon for the IT business domain in both English and Chinese.', 'A dictionary-based approach', 'A dictionary-based approach has been adopted that combines phrasal translation, co-occurrence analysis, and pre- and post-translation query expansion\r\n<br/>\r\nPhrasal translations and co-occurrence analysis were also implemented.', 'All methods except word-by-word get over 70% performance in monolingual system.<br/>\r\nPhrasal and co-occurrence disambiguation performed much better than word-by-word translation, achieving a 74.6% improvement.\r\n<br/>\r\nPre and post-translation expansion did not improve the performance.', '7'),
(2, 'Improving Interactive Retrieval by Combining Ranked Lists and Clustering', 'Anton Leuski, James Allan', 2004, 'Organizing the documents', 'Organizing the documents returned by an information retrieval system in response to a natural language query. ', 'Interactive Retrieval', 'TREC ad-hoc queries, relevance judgments', 'TREC ad-hoc queries that ran against the documents in TREC volumes 2 and 4 (2.1GB) that include articles from Wall Street Journal, Financial Times, and Federal Register.<br/> \r\nRelevance judgments supplied by NIST accessors (Harman & Voorhees, 1997)', 'Ranked list, clustering of the results', 'Ranked list, clustering of the results.\r\nevaluate by comparing to the ranked list and interactive relevance feedback search strategies.', 'Significantly exceeds the initial performance of the ranked listand rivals in its effectiveness the traditional relevance feedback methods.', ''),
(3, 'Using Citations to Generate surveys of Scientific Paradigms', 'Saif Mohammad, Bonnie Dorr, Melissa Egan, Ahmed Hassan,Pradeep Muthukrishan, Vahed Qazvinian, Dragomir Radev, David Zajic', 2009, 'Automatically generated survey', 'First steps in producing an automatically generated, readily consumable, technical survey.', 'Generate surveys', 'ACL Anthology', 'The ACL Anthology is a collection of papers.\r\n<br/>\r\nEvaluation experiments are on a set of papers in the research area of Question Answering (QA)and another set of papers on Dependency parsing(DP).', 'Bibliometric lexical link mining and summarization techniques', 'Combining bibliometric lexical link mining and summarization techniques<br/>\r\nFour summarization systems for survey-creation approach: Trimmer,LexRank, C-LexRank, and C-RR.<br/>\r\nFor this we evaluated using : nugget-based pyramid evaluation and ROUGE', 'Both citation texts and abstracts have unique survey-worthy information. Multidocument summarization especially technical survey creation—benefits considerably from citation texts.', ''),
(4, 'Multi-document Summarization by Graph Search and Matching', 'Inderjeet Mani, Eric Bloedorn', 1997, 'Summarize multiple documents', 'Summarize the similarities and differences in information content among multiple documents in a way that is sensitive to the needs of the user.', 'Graph Search and Matching', 'Four queries from the TREC (Harman 1994)', 'Four queries from the TREC (Harman 1994) collection of topics, with the idea of exploiting their associated (binary) relevance judgments.\r\n<BR/>\r\nFor this experiment, 15 pairs of articles on international events were selected from searches on the World Wide Web.', 'Graph representation', 'Graph representation by making abstract content representation based on explicitly representing entities and the relations between entities, of the sort that can be robustly extracted by current information extraction systems. ', 'Summaries were used, the performance was faster than with fulltext (F=32.36, p < 0.05, using analysis of variance F-test) without significant loss of accuracy. <br/>\r\nShorter texts are effective enough to support accurate retrieval.<br/>\r\nThe biggest improvement comes from the differences found using spreading', ''),
(5, 'Resolving Ambiguity for Cross-language Retrieval', 'Lisa Ballesteros, W. Bruce Croft', 1998, 'Resolving Ambiguity for Cross-language Retrieval', 'One of the main hurdles to improved CLIR effectiveness is resolving ambiguity associated with translation.\r\n\r\nResources for cross-language retrieval can require tremendous manual effort to generate and may be difficult to acquire. Therefore methods which capitalize on existing resources must be\r\nfound.', 'Resolving Ambiguity', 'TREC AP English', 'Evaluation was performed on the 748 MB TREC AP English collection (having 243K documents covering ’88-’90) with provided relevance judgments. Co-occurrence statistics were collected from the portion of the AP collection covering 1989.', 'Cooccurrence statistics, parallel corpus analysis', 'Technique based on cooccurrence statistics from unlinked corpora which can be used to reduce the ambiguity associated with phrasal and term translation. \r\n<br/>\r\nEmploy parallel corpus analysis to look at the impact of query term disambiguation on CLIR effectiveness. ', 'Combining corpus analysis techniques can be used to disambiguate terms and phrases. In combination with query expansion, it significantly reduces the error associated with query translation. Techniques based on unlinked corpora can perform as well or better than techniques based on more complex or scarce resources. Our co-occurrence method was better at disambiguating queries than was our parallel corpus technique.', ''),
(6, 'Word Sense Disambiguation with a Corpus-based Semantic Network', 'Qujiang Peng, Takeshi Ito, Teiji Furugori ', 1996, 'WSD', 'Determining the meaning of words in text is by Word sense disambiguation (WSD)', 'WSD', '10 polysemous words', 'We tested our method for the same 10 \r\npolysemous words used in our previous \r\nwork with 756 instances collected and other materials.', 'Corpus-based semantic network', 'Uses a corpus-based semantic network.   Creating a semantic network that represents semantic distances among words in general, we resolve the ambiguities activating the network. ', 'Achieved a success rate of 92.1%, which is better than those of other comparable', ''),
(7, 'Word Sense Disambiguation with Automatically Acquired Knowledge', 'Ping Chen, Wei Ding, Max Choly, Chris Bowes', 2012, 'WSD that applied in any real world applications', 'WSD that applied in any real world applications with automatically acquired knowledge', 'Automatically Acquired Knowledge', 'Senseval-2 fine-grained English testing corpus and SemEval 2007 Task 7 coarse-grained testing corpus. ', 'Testing with two large scale WSD evaluation corpora, Senseval-2 fine-grained English testing corpus and SemEval 2007 Task 7 coarse-grained testing corpus. ', 'Automatically Acquired Knowledge', '1) Corpus building through search engines<br/>\r\n2) Document cleaning<br/>\r\n3) Sentence segmentation<br/>\r\n4) Parsing<br/>\r\n5) Dependency relation merging<br/>\r\n6) Dependency relation normalization<br/>', 'Achieves 82.64% in both precision and recall, which clearly outperforms the best unsupervised WSD system and performs similarly as the best supervised system<br/>\r\n\r\nOur WSD method overcomes the knowledge acquisition bottleneck faced by many current WSD systems. Our main finding is the “greater-sum” disambiguation capability of these three knowledge\r\nsources,the SemEval-2007 and Senseval-2 corpora', ''),
(8, 'An Efficient Linear Text Segmentation Algorithm Using Hierarchical Agglomerative Clustering', 'Ji-Wei Wu', 2011, 'Efficient linear text \r\nsegmentation', 'Efficient linear text \r\nsegmentation algorithm based on hierarchical agglomerative \r\nclustering', 'Text Segmentation', 'Test corpus', 'Test corpus consists of 700 samples. A \r\nsample is a concatenation of ten text segments. The 700 samples are divided into 4 sets according to the range of the number of sentences', 'Hierarchical learning strategy', 'Tokenization, stopword removal, \r\nand stemming. After text preprocessing, the text can be represented \r\nas vectors, each of which represents a sentence within the \r\ntext. A part of sentence similarities are then computed to \r\nconstruct the sentence-similarity matrix. Finally, the optimal \r\ntopic boundaries are identified by the proposed algorithm. ', 'Linear text segmentation \r\nalgorithm (i.e., TSHAC) outperforms the linear time algorithm, TextTiling. TSHAC also provides comparable results with other algorithms. TSHAC provides a fully automatic process for linear text segmentation without auxiliary knowledge base, parameter setting, or user involvement.', ''),
(9, 'The Effects of Query Structure and Dictionary Setups in Dictionary-Based Cross-language Information Retrieval', 'Ari Pirkola', 1998, 'Effects of query structure and various setups of translation dictionaries on the performance of cross-language information retrieval (CLIR)', 'In this study, the effects of query structure and various setups of translation dictionaries on the performance of cross-language information retrieval (CLIR) were tested.\r\n', 'Dictionary-Based Information Retrieval', 'AP Newswire, Federal Register, DOE abstracts', 'The test collection consisted of the AP Newswire, Federal Register, and DOE abstracts subsets of the TREC collection.', 'Combine general language MRD and a domain specific MRD\r\n', 'The translation polysemy and the dictionary coverage problems were attacked by means of the combination of a general language MRD and a domain specific MRD\r\n.', 'There was only a slight difference in performance between the original English queries and the best cross-language queries.\r\n<br/>\r\nA cross-language IR system based on MRD translation is able to achieve the\r\nperformance level of a monolingual system, if queries are structured and if both general terminology and domain spe-\r\ncific terminology are available in translation.', ''),
(10, 'A New Approach to Feature Selection in Text Classification', 'Yi Wang, Xiao-Jing Wang', 2005, 'New approach to feature selection to do feature reduction', 'New approach to feature selection to do feature reduction, which is a constituent process in representing texts.', 'Feature Selection', 'Chinese text classification corpus', 'Divide the corpus into two non-intersected sets: a training set containing 10 categories with 100 texts in each and a test set containing the same 10 categories with another 100 texts in each also', 'Variance-mean based feature filtering', 'Variance-mean based feature filtering method of feature selection to do feature reduction in the representation phase.', 'Variance-mean method can gain higher performance at a very low dimension, and quickly reach a peak, which means much less computing time and almost best performance than DF, CHI.', ''),
(11, 'Text Clustering with Feature Selection by Using Statistical Data', 'Yanjun Li, Congnan Luo, Soon M. Chung', 2008, 'Extended the X2 term-category indepen-\r\ndence test', 'Extended the X2 term-category independence test by introducing new statistical data that can measure whether the dependency between a term and a category is positive or negative, developed a new supervised feature selection method, named CHIR, which is based on the X2 statistic and the new term-category dependency measure.', 'Text Clustering', 'Five test data sets(CACM, MED, EXC, PEO and TOP)', 'Two data sets,CACM and MED, are extracted from the CACM and MEDLINE abstracts, respectively, which are included in the Classic database. Additional three data sets, EXC, PEO and TOP,are from the EXCHANGES, PEOPLE and TOPICS category setsof the Reuters-21578 Distribution 1.0', 'TCFS', 'Text Clustering with Feature Selection (TCFS), which performs the clustering and the supervised feature selection alternately until convergence.', 'CHIR consistently out-performs other three methods in terms of increasing the cohesiveness values of the clusters.', ''),
(12, 'Clustering-Based Feature Selection in Semi-supervised Problems', 'Ianisse Quinzán, José M. Sotoca, Filiberto Pla ', 2009, 'Unlabeled information can improve significant classification result', 'Unlabeled information can improve significant classification result', 'Clustering Feature Selection', 'Gisette, Optdigits, covtype, hyperspectral image', 'Gisette is a big data in the UCI repository, with 5000 attributes and 13500 objects, 7000 of them labelled. Optdigits problem is about the recognition of a handwritten number. The database has 5620 samples and 64 features.Covtype database, the objective is predicting forest \r\ncover type from cartographic variables, with no remotely sensed data. This database has 54 features, 581012 objects and 7 classes. A hyperspectral image called 92AV3C corresponding to a spectral image (145 x 145 pixels, 220 bands, and 17 classes).', 'Hybrid method (combines supervised and \r\nunsupervised measures of information)', 'A new hybrid method for semi-supervised \r\nproblem which combines supervised and unsupervised measures of information. This approach applies a strategy to obtain a feature subset through clustering techniques.', 'The unsupervised information improves the accuracy and the ssfc method is adequate.\r\nOptdigits is a database where sup technique gets high-quality features for few labeled samples. Thus, in this case \r\nthe ssfc has similar performance than sup. Nevertheless when the number of labeled samples is increased, ssfc and sup become similar to supT. ', ''),
(13, 'A Discrete Particle Swarm Optimization Algorithm for Domain Independent Linear Text Segmentation', 'Ji-Wei Wu, Judy C.R. Tseng, Wen-Nung Tsai ', 2010, 'Improve the performance of linear text segmentation', 'Improve the performance of linear text segmentation', 'Discrete Particle Swarm', 'GCE-2004 dataset', 'Choi test corpus consists of 700 samples. A sample is a concatenation of ten text segments and each segment is the first in sentences of a randomly selected document from the Brown corpus.', 'DPSO-SEG', 'The goal of DPSO-SEG is to identify the optimal topic boundaries of the text segments in a document.  At first, the \r\nterms within each sentence are tokenized and stemmed. Then, generic stop words are removed.  After the basic \r\npreprocessing, each sentence is represented as a term-frequency vector. Then, sentence-sentence similarity \r\nbetween a pair of sentences is computed by cosine similarity. A sentence similarity matrix of the text then constructed using the sentence-sentence similarity. Finally, the optimal \r\nboundaries are created by DPSO according to the sentence similarity matrix. ', 'The value of Pk is reduced sharply with fewer numbers of iterations, and smoothly after 350 iterations. It is converged at about 1500 iterations. the performance of DPSO-SEGC99 is better than DPSO-SEG. DPSO-SEGC99 also converges faster.', ''),
(14, 'First Order Statistics Based Feature Selection: A Diverse and Powerful Family of Feature Seleciton Techniques', 'Taghi Khoshgoftaar, David Dittman, Randall Wald, and Alireza Fazelpour', 2012, 'First Order Statistics (FOS) based feature selection', 'First Order Statistics (FOS) based feature selection using seven related univariate\r\nfeature selection metrics', 'First Order Statistics', 'DNA microarray', 'The datasets are all DNA microarray datasets acquired from a number of different real world bioinformatics, genetics, and medical projects. Use datasets with two classes for example:\r\ncancerous/non-cancerous or relapse/no relapse). ', 'Datasets, feature subset size, similarity measure, and classification', 'Datasets, feature subset size, similarity measure, and classification', 'Twenty one possible pairwise comparisons only one combination is above a 0.7 similarity across all twelve feature subset sizes: Fold Change Difference and SAM. Outside of this pair only four other pairs (S2N and Welch T Statistic, Signal to Noise and SAM, Fold Change Difference and Fisher Score, and Welch T Statistic and SAM) achieve a similarity score above 0.7 and only the combination of Welch T Statistic and Fisher Score achieves this below a feature subset size of 500', ''),
(15, 'Relation extraction using dependency parse trees', 'Katrin Fundel, Robert Ku¨ffner, Ralf Zimmer', 2007, 'Relation extraction from free text', 'The use of dependency parse trees as a means for biomedical relation extraction from free text. It is based on natural language preprocessing producing dependency parse trees and applying a small number of simple rules to these trees. ', 'Relation Extraction', 'Synonym dictionary for genes/proteins', 'Synonym dictionary for genes/proteins, a training set (55 sentences and 103 interactions) and a test set (80 sentences and 54 interactions).', 'Effector-relation-effectee, relation-of-effectee-by-effector, relation-between-effector-and-effectee', '(1) effector-relation-effectee (‘A activates B’)\r\n(2) relation-of-effectee-by-effector (‘Activation of A by B’)\r\n(3) relation-between-effector-and-effectee (‘Interaction between A\r\nand B’).', 'HPRD, even though being a very large\r\nand valuable source for protein interaction data, currently covers\r\nonly a small part of the human protein-protein relations from very limited relation categories. RelEx provides complementary information.', ''),
(16, 'Exploiting Constituent Dependencies for Tree Kernel-Based Semantic Relation Extraction', 'Longhua Qian   Guodong Zhou   Fang Kong   Qiaoming Zhu   Peide Qian ', 2010, 'Dynamically determine the tree span for relation extraction by exploiting constituent dependencies', 'Dynamically determine the tree span for relation extraction by exploiting constituent dependencies to integrate dependency information, which has been proven very useful to relation extraction, with the structured syntactic information to construct a concise and effective tree span specifically targeted for relation extraction. Explore interesting combined entity features for relation extraction via a unified parse and semantic tree. ', 'Constituent Dependencies', 'GCE-2004 dataset', 'ACE RDC 2004 corpus as the benchmark data that contains 451 documents and 5702 relation instances. It defines 7 entity types, 7 major relation types and 23 subtypes', 'Condense NounPhrases (NPs)\r\n', '(1) Modification within base-NPs \r\n(2) Modification to NPs\r\n(3)Arguments/adjuncts to verbs\r\n(4)Coordination conjunctions\r\n(5)Modification to other constituents', 'the improvements of different tree setups over SPT. DSPT performs best among DSPT, SPT, CS-SPT. It also shows that the Unified Parse and Semantic Tree with Feature-Paired Tree perform significantly better than the other two tree setups (i.e., CS-SPT and DSPT).', ''),
(17, 'Exploiting Background Knowledge for Relation Extraction', 'Yee Seng Chan and Dan Roth', 2010, 'Supervised RE', 'Improve the performance of RE by considering the relationship between our relations of interest, as well as how they relate to some existing knowledge resources', 'Background Knowledge', 'GCE-2004 dataset', 'ACE-2004 dataset (catalog LDC2005T09 from the Linguistic Data Consortium) to conduct our experiments. ACE-2004 defines 7 coarse-grained relations and 23 fine-grained relations', 'Coarse-grained predictions', 'Using the coarse-grained predictions which should intuitively be more reliable, to improve the fine-grained predictions.Using Novel to contrain the predictions of the fine-grained.', 'Performing the usual evaluation on mentions gives similar performance figures. All the background knowledge helped to improve performance, providing a total improvement of 3.9 to our basic RE system. Improves the performance of coarse-grained relation predictions.', ''),
(18, 'Automatic Evaluation of Relation Extraction Systems on Large-scale', 'Mirko Bronzi, Zhaochen Guo, Filipe Mesquita', 2012, 'Framework for large-scale evaluation of relation extraction systems', 'Framework for large-scale\r\nevaluation of relation extraction systems based on an automatic annotator that uses a public online database and a large web corpus.', 'Automatic Evaluation', 'ReVerb and SONEX', 'Compare two open RE systems: ReVerb and SONEX. The input corpus for this comparison is the New York Times corpus, composed by 1.8 million documents. ReVerb  extracts relational phrases using rules over part-of-speech tags and noun-phrase chunks.', 'Automatic annotator', 'Use of an automatic annotator: a system capable of verifying whether or not a fact was correctly extracted. This is done by leveraging external sources of data and text, which are not available to the systems being evaluated', 'About 63 million facts in G'', the superset of the ground truth G. ', ''),
(19, 'Confidence Estimation Methods for Partially Supervised Relation Extraction', 'Eugene Agichtein', 1992, 'Extract structured relations between named entities', 'Extract structured relations between named entities (e.g., a company name, a location name, or a name of a drug or a disease) from unstructured documents with minimal human effort. ', 'Partially Supervised Relation Extraction', 'Three relations extracted', 'Three relations extracted from a collection of 145,000 articles from the New York Times from 1996, available as part of the North American News Text Corpus1.', 'Expectation Maximization (EM)', 'Expectation Maximization (EM) algorithms for estimating pattern and tuple confidence.', 'The EM-based methods have higher accuracy than the constraint-based method', ''),
(20, 'A New Approach to Word Sense Disambiguation Based on Context Similarity ', 'M. Nameh, S.M. Fakhrahmad, M. Zolghadri Jahromi', 2011, 'Improve accuracy in WSD', 'The human mind is able to select the proper target equivalent of any source \r\nlanguage word by comprehension of the context.\r\n<br/>\r\nIn order to simulate this behavior in a machine, a huge amount of data will be required as input and the output may still not be free from errors.', 'Context Similarity ', 'TWA sense tagged data', 'In order to evaluate the proposed scheme, we used TWA sense tagged data which is a benchmark corpus developed at University of North Texas by Mihalcea and Yang in 2003. ', 'Inner product of vectors algorithm', 'Inner product of vectors algorithm. The proposed scheme is a supervised approach in which sense-tagged data is used to train the classifier.', 'The results are promising compared to the methods existing in the literature and were encouraging in most cases.  ', ''),
(21, 'Unsupervised Word Sense Disambiguation Using Neighborhood Knowledge', 'Huang Heyan, Yang Zhizhuo, Jian Ping', 2011, 'context window size', 'It has been proved that expanding context window size around the target ambiguous word can help to enhance the WSD performance. However, expanding window size unboundedly will bring not only useful information but also some noise which may finally deteriorate the WSD performance. ', 'Neighborhood Knowledge', 'Sogou Chinese collocation relation', 'In the experiment, Sogou Chinese collocation relation was used to compute mutual information of words. ', 'Hierarchical learning strategy', 'This study proposes to construct an appropriate knowledge context for unsupervised WSD method by making use of a few neighbor sentences closed to the ambiguous sentence in the article.', 'The neighborhood knowledge can significantly improve the performance of single sentence WSD.', ''),
(32, 'Semi-supervised Relation Extraction with Large-scale Word Clustering', 'Ang Sun, Ralph Grishman, Satoshi Sekine', 2011, 'Semi-supervised Relation Extraction with Large-scale Word Clustering', 'Cluster-based features in a systematic way ,several statistical methods for selecting effective clusters, impact of the size of training data on cluster features, analyze the performance improvements through an extensive experimental study.', 'Word Clustering', 'ACE 2004', 'We used TDT5 unlabeled data for inducing word clusters. It contains roughly 83 million words in 3.4 million sentences with a vocabulary size of 450K. We induced 1,000 word clusters for words that appeared at least twice in the corpora. The reduced vocabulary contains 255K unique words.For relation extraction, we used the benchmark ACE 2004 training data. ', 'Hierarchical learning strategy', 'Simple learning framework that is similar to the hierarchical learning strategy. Specifically, train a binary classifier to distinguish between relation instances and non-relation instances. Then use only the annotated relation instances to train a multi-class classifier for the 7 relation types.', 'Zhou et al.(2007) : 78.2(P%) 63.4(R%) 70.1(F%)<BR/>\r\nZhao and Grishman (2005) : 69.2(P%) 71.5(R%) 70.4(F%)<BR/>\r\nOur Baseline : 73.4(P%) 67.7(R%) 70.4(F%)\r\nJiang and Zhai(2007) : 72.4(P%) 70.2(R%) 71.3(F%)', ''),
(33, 'Feature Selection and Feature Extraction for Text Categorization', 'David D. Lewis', 1992, 'Words properties, syntactic phrase indexing and term clustering', 'The effect of selecting varying numbers and kinds of features for use in predicting category membership was investigated on the Reuters and MUC-3 text categorization datasets. The extraction of new text features by syntactic analysis and feature clustering was investigated on the Reuters data set. However, words have properties such as synonymy and polysemy, that make them a less than ideal indexing language. Syntactic phrase indexing and term clustering have opposite effects on the properties of a text representation, which led us to investigate combining the two techniques.', 'Text categorization, syntactic parse, syntactic relationships, syntactic indexing phrase', 'Reuters data set, US. MUC-3 data set', 'Our first data  set was a set of 21,450 Reuters newswire stories from the year 1987. The second data set consisted of 1,500 documents from the US.  Foreign Broadcast Information Service (FBIS) that had previously been used in the MUC-3 evaluation of natural language processing systems.', 'Statistical model', 'The statistical model used in our experiments was proposed by Fuhr for probabilistic text retrieval, but the adaptation to  text categorization is straightforward. Each category is assigned to its top scoring documents on the test set in a designated multiple of the percentage of documents it was assigned to on the training corpus. For the Reuters data we adopted a conservative approach to syntactic phrase indexing.', 'The optimal feature set size for word-based indexing was found to be surprisingly low (10 to 15 features) despite the large training sets. Syntactic indexing phrases, clusters of these phrases, and clusters of words were all found to provide less effective representations than individual words. ', ''),
(34, 'Page Segmentation and Classification using Fast Feature Extraction and Connectivity Analysis', 'Jaakko Sauvola and Matti Pietikainen', 1995, 'Document structure analysis', 'Document structure analysis and understanding are the main processes in reaching this goal: ease of use and availability of documents. In order to achieve the best possible results with OCR and storaging, the contents of the document have to be examined.', 'Page Segmentation and Classification, Connectivity Analysis', 'Sample images', 'The text blocks have been extracted, the picture blocks and the background is ignored in the text extraction process. Pictures can be extracted by defining them as targets for extraction in the produced command file.', 'PCS (Page, Classification, Segmentation) Procedure', 'The integration process includes fast feature extraction with rule-based classification and label propagation using connectivity analysis providing classified areas in three categories: background, text and picture. The PCS procedure combines the segmentation and classification parts.', 'The percentage of the extracted text was 99-100% of all text in test images.', ''),
(35, 'Should we Translate the Documents or the Queries in Cross-language Information Retrieval?', 'J. Scott McCarley ', 1999, 'Document or queries to be translated?', 'Previous comparisons of document and query translation suffered difficulty due to  differing quality of machine translation in these two opposite directions.', 'Document, Query, Translation, Cross-language', 'TREC-6 and TREC-7 CLIR tracks', 'The English document set consisted of 3 years of AP newswire (1988-1990), comprising 242918 stories originally occupying 759 MB. The French document set consisted of the same 3 years of SDA (a Swiss newswire service), comprising 141656 stories and originally occupying 257 MB.', 'Hybrid systems', 'We avoid this difficulty by training identical statistical translation models for both translation directions using the same training data. We investigate information retrieval between English and French, incorporating both translations directions into both document translation and query translation-based information retrieval, as well as into hybrid systems.', 'We find that hybrids of document and query translation-based systems outperform query translation systems, even human-quality query translation systems', ''),
(36, 'Multi-Document Summarization By Sentence Extraction ', 'Jade Goldstein, Vibhu Mittal t Jaime Carbonell, Mark Kantrowitzt ', 2000, 'Multi-document summarization, large scale IR', 'Multi-document summarization differs from single in that the issues of compression, speed, redundancy and passage selection are critical in the formation of useful summaries. However, large scale IR and summarization have not yet been truly integrated, and the functionality challenges on a summarization system are greater in a true IR or topic-detection context (Yang et al., 1998; Allan et al., 1998). ', 'Multi-document, summarization, information extractions', 'NTCIR-1 (NACSIS Test Collection 1)', 'It contains summaries of papers presented at conferences hosted by 65 Japanese academic societies, and we used E-Collection, which contains about 190,000 English summaries.', 'Domain independent techniques', 'Our approach addresses these issues by using domain independent techniques based mainly on fast, statistical processing, a metric for reducing redundancy and maximizing diversity in the selected passages, and a modular framework to allow easy parameterization for different genres, corpora characteristics and user requirements. ', 'Counting clearly distinct propositions in both cases, yields a 60% greater information content for the MMRMD case, though both summaries are equivalent in length. When these 200 documents were added to a set of 4 other topics of 200 documents, yielding a document-set with 1000 documents, the query relevant multi-document summarization system produced exactly the same resuits. ', ''),
(37, 'TextTiling: A Quantitative Approach to Discourse Segmentation', 'Marti A. Hearst', 2000, 'Recognition of structure', 'Expository texts such as science magazine articles and environmental impact reports can be viewed as being composed of a few main topics and a series of short, sometimesdensely discussed, subtopics. The capability to automate the recognition of this kind of structure in a full-text document should be useful for improving a variety of computational tasks, e.g., hypertext, text summarization and information retrieval.', 'TextTiling, Lexical Analyses', 'NTCIR-1 (NACSIS Test Collection 1)', 'It contains summaries of papers presented at conferences hosted by 65 Japanese academic societies, and we used E-Collection, which contains about 190,000 English summaries.', 'TextTiling, lexical analyses', 'The approach uses lexical analyses based on tf.idf, an information retrieval measurement, to determine the extent of the tiles, incorporating thesaural informationvia a statistical disambiguation algorithm. Unlike standard discourse analysis approaches, TextTiling breaks the text into simple, contiguous tile that are meant to reflect only topical loci, and not the interrelations among the topics.', 'We show that using the mode topic ID assigned during the inference method of LDA, used to annotate unseen documents, improves performance by stabilizing the obtained topics. We show significant improvements over state of the art segmentation algorithms on two standard datasets. As an additional benefit, TopicTiling performs the segmentation in lin- ear time and thus is computationally less expensive than other LDA-based segmentation methods.', ''),
(38, 'Implementation of an Automated Text Segmentation System using Hearst''s TextTiling Algorithm', 'Brent Fitzgerald', 2000, 'Subtopic detection', 'Most of the texts one comes across are composed of a number of topics, perhaps varying in their relevance to one another and their scope. A system that could automatically detect these subtopics would certainly be useful, allowing the reader to quickly skip to the topics most relevant to her purpose.', 'Subtopic detection, TextTiling', 'NTCIR-1 (NACSIS Test Collection 1)', 'It contains summaries of papers presented at conferences hosted by 65 Japanese academic societies, and we used E-Collection, which contains about 190,000 English summaries.', 'Hearst''s TextTiling algorithm', 'The approach used in this paper is based on Hearst''s TextTiling algorithm, a moving window approach that uses lexical overlap as a means of detecting topic coherence. ', 'The best precision score was 0.77 when run on the New York Times texts, and it was accompanied by a recall score of 0.77 as well.  While these scores may sound relatively impressive, it is important to note that they were only numerically evaluated on this one set of data, and so it is unlikely that those parameters would return such high scores in all circumstances.', ''),
(39, 'Query Term Disambiguation for Web Cross-Language Information Retrieval using a Search Engine', 'Akira Maeda, Fatiha Sadat, Masatoshi Yoshikawa, and Shunsuke Uemura', 2000, 'Queries translation ', 'Existing CLIR approaches based on query translation require parallel corpora or comparable corpora for the disambiguation of translated query terms. However, those natural language resources are not readily available. One of the major technical problems to be solved in CLIR concerns the translation of short queries of one or few words, appropriately. Possible translation-candidates might be numerous in such cases and resolving such ambiguities becomes a hard task.', 'Cross-language, Query Translation', 'NTCIR-1 (NACSIS Test Collection 1)', 'It contains summaries of papers presented at conferences hosted by 65 Japanese academic societies, and we used E-Collection, which contains about 190,000 English summaries.', 'Co-occurence information', 'In this paper, we propose a novel approach for CLIR system targeting Web documents, which uses a natural language resource that is extracted from a Web search engine as a corpus, and resolves the ambiguities caused by the dictionary-based query translation approach, by using a co-occurrence information.', 'In the experiments, our method achieved 97% of manual translation case in terms of the average precision.', ''),
(40, 'Query Translation Method for Cross Language Information Retrieval', 'Hiroshi Masuichi, Raymond Flournoy, Stefan Kaufmann, Stanley Peters', 2000, 'Query translation', 'This paper proposes a method of query translation for Cross Language Information Retrieval. A cross language information retrieval (CLIR) system is a system for retrieving documents across language boundaries. A query written in one language should be translated into a representation for finding documents in another language.', 'Cross-language, Query Translation', 'NTCIR-1 (NACSIS Test Collection 1)', 'It contains summaries of papers presented at conferences hosted by 65 Japanese academic societies, and we used E-Collection, which contains about 190,000 English summaries.', 'Parallel bilingual corpus', 'The method uses a parallel bilingual corpus to produce word vectors and can readily be applied to monolingual vector-retrieval models. Our basic idea for applying the information mapping approach to CLIR is to produce a vector for a word in one language L with content-bearing words in another language L''.  We use a bilingual parallel corpus to produce the word vectors.', 'All four tables show good results; a total of 1559 out of 1600 (97.4%) queries retrieved their counterpart patents at the first rank. A total of 735 out of 800 (91.9%) queries retrieved their counterpart patents at the first rank.', ''),
(41, 'A New Approach to Unsupervised Text Summarization', 'Tadashi Nomoto, Yuji Matsumoto', 2001, 'Assumption that human-made summaries are reliable enough. The portability of a summarization system which large amount of data need to be collected, manually annotated, and train the system', 'One of the problems with the former approach has to do with its underlying assumption that human-made summaries are reliable enough to be used as gold standard for automatic summarization. Another problem associated with the approach concerns the portability of a summarization system: deploying the system in a new domain usually requires one to collect a large amount of data, which need to be manually annotated, and then train the system.', 'Text Summarization, Clustering, MDL, BMIR-J2', 'BMIR-J2 corpus', 'A test data developed by a Japanese research consortium.', 'Diversity-based approach', 'The novelty lies in exploiting the diversity of concepts in text for summarization, which has not received much attention in the summarization literature. A diversity-based approach here is a principled generalization of Maximal Marginal Relevance criterion by Carbonell and Goldstein. We propose, in addition, an information-centric approach to evaluation, where the quality of summaries is judged not in terms of how well they match human-created summaries but in terms of how well they represent their source documents in IR tasks such document retrieval and text categorization.', 'a clear superiority of a diversity based approach to a non-diversity based approach.', ''),
(42, 'Improving Query Translation for Cross-Language Information Retrieval using Statistical Models', 'Jianfeng Gao, Jian-Yun Nie, Endong Xun, Jian Zhang, Ming Zhou, Changning Huang', 2001, 'Translation ambiguity', 'We are faced with the problem of translation ambiguity, i.e. multiple translations are stored in a dictionary for a word. In addition, a word-by-word query translation is not precise enough.', 'Query Translation, Cross-language, Statistical Models', 'BMIR-J2 corpus', 'A test data developed by a Japanese research consortium.', 'Statistical models, phrase translation patterns, translation ambiguity problem', 'First, as many as possible, noun phrases are recognized and translated as a whole by using statistical models and phrase translation patterns. Second, to deal with the translation ambiguity problem, we propose a method based on statistics of co-occurrences. Finally, to increase the coverage of the bilingual dictionary, additional words and translations are automatically generated from a parallel bilingual corpus.', 'In summary, the improvement by using NP translation for short queries is statistically significant (p-value = 0.015). The addition of translation selection is also statistically significant for long queries (p-value = 0.05).', ''),
(43, 'An Evaluation on Feature Selection for Text Clustering', 'Tao Liu, Shengping Liu, Zheng Chen', 2003, 'Only unsupervised feature selection can be exploited', 'But in real case the class information is unknown, so only unsupervised feature selection can be exploited. In many cases, unsupervised feature selection are much worse than supervised feature selection, not only less terms they can remove, but also much worse clustering performance they yield.', 'Feature Selection, Text Clustering', 'R2, 20NG, Web Corpus', 'So we chose three different text datasets to evaluate text clustering performance, including two standard labeled datasets: Reuters-21578 1  (Reuters), 20 Newsgroups 2 (20NG), and one web directory dataset (Web) collected from the Open Directory Project3. ', 'Term Contribution', 'Then we propose a new  feature selection method called Term Contribution (TC) and perform a comparative study on a variety of feature selection methods for text clustering, including Document Frequency (DF), Term Strength (TS), Entropy-based (En), Information Gain (IG) and ×?2 statistic (CHI). Finally, we propose an â€œIterative Feature Selection (IF) method that addresses the unavailability of label problem by utilizing effective supervised feature selection method to iteratively select features and perform clustering. ', 'To prove this, we conducted a Naive Bayesian classification experiment on Web Directory dataset and found that the classification accuracy increased from 49.6%  to 57.6% after removing 98% terms.', ''),
(44, 'LexRank: Graph-based Lexical Centrality as Salience in Text Summarization', 'Gua nesa¸ Erkan; Dragomir R. Radev', 2004, 'Multi-document extractive generic text summarization', 'We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. In this paper, we focus on multi-document extractive generic text summarization, where the goal is to produce a summary of multiple documents about the same, but unspecified topic.', 'Multi-document, Unspecified Topic, Sentence Salience', 'Document Understanding Conferences (DUC) 2003 and 2004', 'Test data for our experiments are taken from 2003 and 2004 summarization evaluations of Document Understanding Conferences (DUC) to compare our system with other state-of-the-art summarization systems and human performance as well.', 'Prestige2 in social networks', 'All of our approaches are based on the concept of prestige2 in social networks, which has also inspired many ideas in computer networks and information retrieval. We hypothesize that the sentences that are similar to many of the other sentences in a cluster are more central (or salient) to the topic.', 'The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank.', ''),
(45, 'The Embra System at DUC 2005: Query-oriented Multi-document Summarization with a Very Large Latent Semantic Space', 'Ben Hachey, Gabriel Murray & David Reitter', 2005, 'Rouge and Pyramid evaluation schemes', 'DUC 2005 investigated both Rouge and Pyramid evaluation schemes in addition to more standard human evaluations of responsiveness and linguistic quality.', 'DUC 2005,  Semantic Space', 'Topic clusters', 'There were 50 topic clusters to be summarised with respect to a short topic query consisting of a 1 to 4 sentence description of an information need. ', 'Embra system', 'The system takes a novel approach to relevance and redundancy, modeling sentence similarity using a latent seman tic space constructed over a very large corpus. The Embra (Edinburgh Multi-document Breviloquence Assay) system is based on a Maximal Marginal Relevance (MMR) framework (Carbonell and Goldstein, 1998), where a single extraction score is derived by combining measures of relevance and redundancy of candidate sentences. The system is novel in that it measures relevance and redundancy using a very large latent semantic space. It addresses specificity by detecting the presence or absence of Named Entities in our extract candidates. And it implements a sentence-ordering algorithm to maximize sentence coherence in our final summaries.', 'The Embra system performance is better than mean and median system scores for the responsiveness measure and for three of the five linguistic quality measures (grammaticality, non-redundancy and focus). It is just below mean and median scores for structure/coherence.', ''),
(46, 'A New Approach to Feature Selection in Text Classification', 'Yi Wang, Xiao-jing Wang', 2005, 'Effectively represent a text', 'Text classification is the process of automatically assigning predefined categories to free text, which is very important to information retrieval and many other applications. Of it, the first important thing is to effectively represent a text to characterize it as belonging to a specified category based on its content and thus make the following phase of classifier training and using more effective and efficient regarding to the final classification performance.', 'Feature Selection, Variance-mean based Feature Filtering', 'Chinese text classification corpus', 'We divide  the corpus into two non-intersected sets: a training set containing 10 categories with 100 texts in each and a test set containing the same 10 categories with another 100 texts in each also.', 'Variance-mean based feature filtering method of feature selection', 'In  this paper, an effective and efficient new method called variance-mean based feature filtering method of feature selection to do feature reduction in the representation phase for text classification is proposed.', 'It keeps the best features, and thus improves the final performance e.g. macro-f1 to 0.92 and simultaneously decreases the computing time for representing the incoming text waiting to be classified dramatically, which is important because it occurs on line and is time-critical.', ''),
(47, 'Hybrid Approach of Query and Document Translation with Pivot  Language for Cross-Language Information Retrieval', 'Kazuaki Kishida, Noriko Kando', 2005, 'Document and query translation', 'Some researchers have already attempted to merge two results from query and document translation for enhancing effectiveness of CLIR. One problem for implementing  this approach is that the document translation is usually a cost-intensive task, but we can alleviate it by using simpler translation techniques, e.g., pseudo translation in which each term is simply replaced with its corresponding translations by a bilingual dictionary.', 'Cross-language, Document Translation, Query Translation', 'French collection', 'The target French collection includes 177,452 documents in total. The average document length is 232.65 words.', 'Hybrid approach, pseudo-translation', 'This paper reports experimental results of cross-language information retrieval (CLIR) from German to French, in which a hybrid approach of query and document translation was attempted, i.e, combining results of query translation (German to French) and of document translation (French to German). In order to avoid too high complexity of computation for translating a large  amount of texts in documents, we executed pseudo-translation, i.e., a simple replacement of terms by a bilingual dictionary (for query translation, a machine translation system was used).', 'The hybrid approach using English documents translated from the original collection (hybrid-2, SrgMgE03) outperforms another hybrid approach using German documents (hybrid-1, SrgdMgG02), i.e., the scores of mean average precision (MAP) are 0.2605 for hybrid-2 and 0.2492 for hybrid-1.', ''),
(48, 'Automatic Extraction of Hierarchical Relations from Text', 'Ting Wang, Yaoyong Li, Kalina Bontcheva, Hamish Cunningham, and Ji Wang', 2006, 'Antology-based applications require methods for automatic discovery', 'Automatic extraction of semantic relationships between entity instances in an ontology is useful for attaching richer semantic metadata to documents. However, in addition to this, many ontology-based applications require methods for automatic discovery of properties and relations between instances. One barrier to applying relation extraction in ontology-based applications comes from the difficulty of adapting the system to new domains.', 'Automatic extraction, semantic relationships', 'ACE 2004', 'ACE 2004', 'SVM based approach', 'In this paper we propose an SVM based approach to hierarchical relation extraction, using features derived automatically from a number of GATE-based open-source language processing tools. In comparison to the previous works, we use several new features including part of speech tag, entity subtype, entity class, entity role, semantic representation of sentence and WordNet synonym set. Motivated by the above work, we use the SVM as well and apply a diverse set of Natural Language Processing (NLP) tools to derive features for relation extraction. In particular, several new features are introduced, such as part-of-speech (POS) tags, entity subtype, entity class, entity role, semantic represen tation of sentences and WordNet synonym set.', 'The results show there is a trade-off among these factors for relation extraction and the features containing more information such as semantic ones can improve the performance of the ontological relation extraction task.', '');
INSERT INTO `data_penelitian` (`id`, `judul`, `peneliti`, `tahun_publikasi`, `masalah`, `deskripsi_masalah`, `keyword`, `domain_data`, `deskripsi_domain_data`, `metode`, `deskripsi_metode`, `hasil`, `creater`) VALUES
(49, 'A TextTiling Based Approach to Topic Boundary Detection in Meetings', 'Satanjeev Banerjee and Alexander I. Rudnicky', 2006, 'Detect boundaries between discussions', 'Our goal is to automatically detect boundaries between discussions of different topics in meetings. Our goal as a part of the CALO project1 is to automatically understand discussions at meetings. A first step towards such understanding is to detect the topics of discussion. This problem can be broken into two parts detecting when there is a change of topic, and determining what the topic is. In this paper we describe our current work on the first question the detection of boundaries between different topics of discussion in meetings. ', 'Texttiling', 'CMU-2, CMU-3 and SRI-1', 'The data we use in this paper consists of 3 sequences of meetings named CMU-2, CMU-3 and SRI-1.', 'Marti Hearsts TextTiling, Beeferman''s adaptive language model', 'We base our algorithm on Marti Hearsts TextTiling algorithm where the probability that a point in a text essay is a topic boundary is computed based on the similarity between the words in windows to the left and right of that point. This algorithm makes very little assumptions about the domain of the text being segmented, which makes it well suited to our application. Other approaches to topic segmentation include that of Beeferman, et. al. who use adaptive language models and cue phrases? (phrases that typically occur near topic boundaries) to segment news transcripts into separate stories.', 'We report average precision of 0.85 and recall of 0.59 when segmenting unseen test meetings. Error analysis of our results shows that although the basic idea of our algorithm is sound, it breaks down when participants stray from typical behavior (such as when they monopolize the conversation for too long). On performing this experiment across all the 6 data combinations, we get an average precision of 0.85, recall of 0.59 and f measure of 0.67 on unseen test data. The exact best window size for the two predictors changes based on the training data; the average being 56 seconds for the speech activity predictor and 70 seconds for the all words predictor. The average best weight for linear combination is 0.6 for the speech activity boundary predictor (and 0.4 for the all words predictor).', ''),
(50, 'Generating and Evaluating Evaluative Arguments', 'Giuseppe Carenini, Johanna D. Moore', 2006, 'Generating and presenting evaluative arguments', 'With the proliferation of on-line systems serving as personal advisors and assistants, there is a pressing need to develop general and testable computational models for generating and presenting evaluative arguments.', 'Evaluative Arguments', 'Questionnaire', 'The user fills out a questionnaire about her attitudes and beliefs about the new instance and the decision task.', 'Additive Multiattribute Value Function (AMVF), Multiattribute Utility Theory (MAUT)', 'GEA, like most previous work in NLG, makes the assumption that deep generation should strictly precede surface generation, and adopts the resulting pipeline architecture. One model that satisfies the requirements noted above is the additive multiattribute value function (AMVF), which is based on multiattribute utility theory (MAUT).', 'The first hypothesis was only marginally confirmed in the experiment (0.05 < p < 0.10), while the second one was confirmed at p < 0.05.', ''),
(51, 'Using Query-Relevant Documents Pairs for Cross-Lingual Information Retrieval', 'David Pinto, Alfons Juan, Paolo Rosso', 2006, 'Techniques for managing of data', 'The development of novel techniques for managing of data, especially when we deal with information in multiple languages, is needed. In Cross-Language Information Retrieval (CLIR), the usual approach is to first translate the query into the target language and then retrieve documents in this language by using a conventional, monolingual information retrieval system.', 'Cross-lingual, Query Relevant', 'EuroGOV corpus', 'EuroGOV corpus which was first used in the bilingual English to Spanish subtask of WebCLEF 2005', 'Query Relevant Document Pairs (QRDP), Transition Point (TP)', 'In this work, we propose to use a training corpus made up by a set of Query Relevant Document Pairs (QRDP) in a probabilistic cross-lingual information retrieval approach which is based on the IBM alignment model 1 for statistical machine translation. The corpus reduction was based on the use of a technique for selecting mid-frequency terms, named the Transition Point (TP), which was used in other research works with the same purpose.', 'An improvement by using an evaluation corpus was obtained employing the TP technique with a neighbourhood of 40%, which is exactly the same percentage used in other research works.', ''),
(52, 'Combining Lexical and Grammatical Features to Improve Readability Measures for First and Second Language Texts', 'Michael J. Heilman, Kevyn Collins-Thompson, Jamie Callan, Maxine Eskenazi ', 2007, 'An automatic measure of readability', 'An automatic measure of readability that incorporated both lexical and grammatical features was thus needed. One goal of this work is to show that the use of pedagogically motivated grammatical features (e.g., passive voice, rather than the number of words per sentence) can improve readability measures based on lexical features alone.', 'REAP tutoring system, first language, second language', 'Web corpus, English as a Second Language Textbook', 'The first corpus was from a set of texts gathered from the Web for a prior evaluation of the language modeling approach. The second corpora consisted of textbook materials (Adelson-Goldstein and Howard, 2004, for level 2; Ediger and Pavlik, 2000, for levels 3 and 4; Silberstein, 2002, for level 5) from a series of English as a Second Language reading courses at the English Language Institute at the University of Pittsburgh.', 'Unigram language model, multinomial Naive Bayes classifier', 'This work evaluates a system that uses interpolated predictions of reading difficulty that are based on both vocabulary and grammatical features. The statistical model used for this study is based on a variation of the multinomial Naive Bayes classifier. The language models employed in this work are simple: they are based on unigrams and assume that the probability of a token is independent of the surrounding tokens. ', 'The results show that for both the first and second language corpora, the language modeling (LM) approach alone produced more accurate predictions than the grammar-based approach alone. The results also indicate that while grammar based predictions are not as accurate as the vocabulary-based scores, they can be combined with vocabulary-based scores to produce more accurate interpolated scores. ', ''),
(53, 'Feature Selection Methods for Text Classification', 'Anirban Dasgupta, Petros Drineas, Boulos Harb, Vanja Josifovski, Michael W. Mahoney', 2007, 'Feature selection', 'We consider feature selection for text classification both theoretically and empirically. Nevertheless, general theoretical performance guarantees are modest and it is often difficult to claim more than a vague intuitive understanding of why a particular feature selection algorithm performs well when it does. Indeed, selecting an optimal set of features is in general difficult, both theoretically and empirically; hardness results are known, and in practice greedy heuristics are often employed.', 'Feature selection', 'TechTC-100, 20-Newsgroups, and Reuters-RCV2 datasets.', 'TechTC-100, 20-Newsgroups, and Reuters-RCV2 datasets.', 'Unsupervised feature selection strategy', 'Our main result is an unsupervised feature selection strategy for which we give worst-case theoretical guarantees on the generalization power of the resultant classification function with respect to the classification function f obtained when keeping all the features. But rather than employing the Singular Value Decomposition (which, upon truncation, would result in a small number of dimensions, each of which is a linear combination of up to all of the original features), we will attempt to choose a small number of these features that preserve the relevant geometric structure in the data (or at least in the data insofar as the particular classification algorithm is concerned).', 'All the selection strategies except document frequency (DF) and uniform sampling (US) achieve 85% of the original (involving no sampling) micro-averaged F1 performance with only 500 out of the (roughly) 20K original features. In general, the subspace sampling (SS) and information gain (IG) strategies perform best, followed closely by weighted sampling (WS).', ''),
(54, 'Unsupervised Relation Extraction from Web Documents', 'Kathrin Eichler, Holmer Hemsen and Guanter Neumann', 2008, 'Information extraction systems and technology', 'Currently, IE systems are usually domain-dependent and adapting the system to a new domain requires a high amount of manual labour, such as specifying and implementing relationâ€“specific extraction patterns manually or annotating large amounts of training corpora. Consequently, current IE technology is highly statically and inflexible with respect to a timely adap tation to new requirements in form of new topics.', 'IE systems', 'Five test data sets(CACM, MED, EXC, PEO and TOP)', 'Two data sets,CACM and MED, are extracted from the CACM and MEDLINE abstracts, respectively, which are included in the Classic database. Additional three data sets, EXC, PEO and TOP,are from the EXCHANGES, PEOPLE and TOPICS category setsof the Reuters-21578 Distribution 1.0', 'Language guesser tool, LingPipe, named entity recognition, coreference resolution', 'In order to restrict the processing to sentences written in English, we apply a language guesser tool, lc4j (Lc4j, 2007) and remove sentences not classified as written in English. To all remaining sentences, we apply LingPipe (LingPipe, 2007) for sentence boundary detection, named entity recognition (NER) and coreference resolution.', 'From the extracted relations, the system built 306 clusters of two or more instances, which were manually evaluated by two authors of this paper. 81 of our clusters contain two or more instances of exactly the same relation, mostly due to the same sentence appearing in several documents of the corpus. Of the remaining 225 clusters, 121 were marked as consistent (i.e., all instances in the cluster express a similar relation), 35 as partly consistent (i.e., more than half of the instances in the cluster express a similar relation), 69 as not use- ful.', ''),
(55, 'Text Clustering with Feature Selection by Using Statistical Data', 'Yanjun Li, Congnan Luo, and Soon M. Chung', 2008, 'Supervised feature selection ', 'Supervised feature selection methods using the information gain and the Ï‡2 statistic can improve the clustering performance better than unsupervised methods when the class labels of documents are available for the feature selection. However, supervised feature selection methods cannot be directly applied to document clustering because usually the required class label information is not available.', 'Feature selection', 'Five test data sets(CACM, MED, EXC, PEO and TOP)', 'Two data sets,CACM and MED, are extracted from the CACM and MEDLINE abstracts, respectively, which are included in the Classic database. Additional three data sets, EXC, PEO and TOP,are from the EXCHANGES, PEOPLE and TOPICS category setsof the Reuters-21578 Distribution 1.0', 'CHIR, TCFS', 'We also developed a new supervised feature selection method, named CHIR, which is based on the Ï‡2 statistic and the new term category dependency measure. Unlike CHI, CHIR selects features having strong positive dependency on the categories. Furthermore, we explored CHIR in text clustering, and developed a new text clustering algorithm, named TCFS, which stands for Text Clustering with Feature Selection. Unlike the IF method, which performs text clustering and feature selection separately, TCFS integrates a supervised feature selection method, such as CHIR, into the text clustering process.', 'Feature selection methods can improve the performance of text clustering as more irrelevant or redundant terms are removed. TCFS with a supervised feature selection method, such as CHIR, CHI or CC, can achieve a better F-measure than k-means with TS.', ''),
(56, 'WikiTranslate: Query Translation for Cross-Lingual Information Retrieval Using Only Wikipedia ', 'Dong Nguyen, Arnold Overwijk, Claudia Hauff, Dolf R.B. Trieschnigg, Djoerd Hiemstra, and Franciska M.G. de Jong ', 2008, 'Cross-lingual information retrieval using Wikipedia', 'The aim of this research is to explore the possibilities of Wikipedia for query translation in CLIR. The main research question of this paper is: Is Wikipedia a viable alternative to current translation resources in cross-lingual information retrieval? This raises the following sub questions: How can queries be mapped to Wikipedia concepts? and how to create a query given the Wikipedia concepts?', 'IR using Wikipedia', 'Five test data sets(CACM, MED, EXC, PEO and TOP)', 'Two data sets,CACM and MED, are extracted from the CACM and MEDLINE abstracts, respectively, which are included in the Classic database. Additional three data sets, EXC, PEO and TOP,are from the EXCHANGES, PEOPLE and TOPICS category setsof the Reuters-21578 Distribution 1.0', 'WikiTranslate, mapping, creating query', 'This paper presents WikiTranslate, a system which performs query  translation for cross-lingual information retrieval (CLIR) using only Wikipedia to obtain translations. We treat Wikipedia articles as representations of concepts (i.e. units of knowledge). The approach used by WikiTranslate consists of two important steps: mapping the query in source language to Wikipedia concepts and creating the final query in the target language using these found concepts.', 'The system achieved a performance of 67% compared to the monolingual baseline. ', ''),
(57, 'A Framework of Feature Selection Methods for Text Categorization', 'Shoushan, Li Rui Xia, Chengqing Zong, Chu-Ren Huang', 2009, 'Suitable feature selection methods', 'However, when  dealing with a new task, it is still difficult to  quickly select a suitable one from various FS  methods provided by many previous studies. However, comparing these FS methods appears to be difficult because they are usually based on different theories or measurements. In order to better understand the relationship between these methods, building a general theoretical framework provides a fascinating perspective. ', 'Feature Selection, Text Categorization', 'R2, 20NG, Cornell movie-review dataset2, Cornell movie-review dataset2', 'In this paper, we propose a framework with two basic measurements for theoretical comparison of six FS methods which are widely used in text classification. Moreover, a novel method is set forth that combines the two measurements and tunes their influences considering different application domains and numbers of selected features. ', 'Two basic measurements: frequency measurement and ratio measurement', 'In this paper, we propose a theoretic framework of FS methods based on two basic measurements: frequency measurement and ratio measurement. Moreover, with the guidance of our theoretical analysis, we propose a novel method called weighed frequency and odds (WFO) that combines the two measurements with trained weights.', 'In the first case when the feature number is low (about less than 1,000), the FS methods in the second group including IG, CHI, WLLR, always perform better than those in the other two groups. In the second case when the feature number is large, among the six traditional methods, MI and BNS take the leads in the domains of 20NG and Movie while IG and CHI seem to be better and more stable than others in the domain of DVD. As for WFO, its performances are excellent cross all these three domains and different feature numbers.', ''),
(58, 'On the Role of Lexical Features in Sequence Labeling', 'Yoav Goldberg and Michael Elhadad', 2009, 'Lexical features', 'We use the technique of SVM anchoring to demonstrate that lexical features extracted from a training corpus are not necessary to obtain state of the art results on tasks such as Named Entity Recognition and Chunking. While early approaches to the NP-chunking task (Cardie and Pierce, 1998) relied on part-of-speech information alone, it is widely accepted that lexical information (word forms) is crucial for building accurate systems for these tasks.', 'Lexical features, sequence labeling', 'Dutch data set from the CoNLL 2002', 'We use the Dutch data set from the CoNLL 2002 shared task (Tjong Kim Sang, 2002). The aim is to identify named entities (persons, locations, organizations and miscellaneous) in text. The task has two stages: identification of the entities, and classification of the identified entities into their corresponding types. We focus here on the identification task.', 'SVM anchoring', 'We use the technique of SVM anchoring to demonstrate that lexical features extracted from a training corpus are not necessary to obtain state of the art results on tasks such as Named Entity Recognition and Chunking. We use the technique of SVM anchoring to demonstrate that lexical features extracted from a training corpus are not necessary to obtain state of the art results on tasks such as Named Entity Recognition and Chunking.', 'For all the sequence labeling tasks we analyzed, the anchored-SVM proved to be robust to feature pruning. The experiments support the claim that rare lexical features do not provide substantial information to the model, but instead play a role in maintaining separability. When this role is taken over by anchoring, we can obtain the same level of performance with very few robust lexical features.', ''),
(59, 'Clustering-based Feature Selection in Semi-supervised Problems', 'Ianisse Quinzan, Josa M. Sotoca, Filiberto Pla', 2009, 'Semi-supervised learning in feature selection', 'The classification or labeling of samples by an expert can often be too expensive in time and sometimes even unfeasible. In many application problems there is available a significant amount of unlabelled data, and only few labeled samples. A challenge in semi-supervised learning is the feature selection problem. ', 'Feature selection, Semi-supervised Learning', 'Gisette datasets', 'Gisette is a big data in the UCI repository, with 5000 attributes and 13500 objects, 7000 of them labelled.', 'Ward''s agglomerative hierarchical clustering', 'Ward''s agglomerative hierarchical clustering has been used as a clustering strategy, but using an adequate distance for the semi-supervised approach. Thus, Ward''s agglomerative hierarchical clustering has been used as a clustering strategy, but using an adequate distance for the semi-supervised approach.', 'Note that the supervised method that uses all labelled samples (supT) always obtain better accuracy than the unsupervised method with all the samples (nsup), except in 92AV3C where both methods have similar performance.', ''),
(60, 'Entity-Focused Sentence Simplification for Relation Extraction', 'Makoto Miwa, Rune,  Yusuke Miyao, Juna ichi Tsujii', 2010, 'The region connecting a pair of entities (in a parsed sentence) is often used to construct kernels or feature vectors that can recognize and extract interesting relations. Such regions are useful, but they can also incorporate unnecessary distracting information.', 'The region connecting a pair of entities (in a parsed sentence) is often used to construct kernels or feature vectors that can recognize and extract interesting relations. Such regions are useful, but they can also incorporate unnecessary distracting information. The shortest paths between a pair of entities (Bunescu and Mooney, 2005) or pair-enclosed trees (Zhang et al., 2006) are widely used as focus regions. These regions are useful, but they can include unnecessary sub-paths such as appositions, which cause noisy features.', 'Relations extraction, rule-based method', 'PPI corpora', 'Protein protein interaction (PPI)', 'Deep parser, Head-driven Phrase Structure Grammar (HPSG), clause-selection\nrule ', 'Our method relies on the deep parser; the rules depend on the Head-driven Phrase Structure Grammar (HPSG) used by Mogura, and all the rules are written for the parser Enju XML output format. A clause-selection rule constructs a simpler sentence (still including both target entities) by removing noisy information before and after the relevant clause. An entity-phrase rule simplifies an entity-containing region without changing the truth-value of the relation.', 'Applying all the rules improved the performance on three of the five corpora, while applying only the clause-selection rules raised the performance for the remaining two corpora as well. ', ''),
(61, 'Collective Cross-Document Relation Extraction Without Labelled Data', 'Limin Yao Sebastian Riedel Andrew McCallum', 2010, 'A novel approach to relation extraction that integrates information across documents, performs global inference and requires no labelled text', 'We present a novel approach to relation extraction that integrates information across documents, performs global inference and requires no labelled text. Naturally, the predictions of a distantly supervised relation extractor will be less accurate than those of a supervised one. While facts of existing knowledge bases are inexpensive to come by, the heuristic alignment to text will often lead to noisy patterns in learning. However, when we use the knowledge base Freebase (Bollacker et al., 2008) and the New York Times corpus (Sandhaus, 2008), we observe very low precision.', 'Relation Extraction, Unlabelled Data', 'Wikipedia, New York Times Corpus', 'An in-domain (Wikipedia) and a more realistic out-of-domain (New York Times Corpus) setting.', 'Joint relation extraction and entity identification', 'In particular, we tackle relation extraction and entity identification jointly. We use distant supervision to train a factor graph model for relation extraction based on an existing knowledge base (Freebase, derived in parts from Wikipedia). For inference we run an efficient Gibbs sampler that leads to linear time joint inference.', '', ''),
(62, 'Interactive Retrieval Based on Faceted Feedback', 'Lanbo Zhang, Yi Zhang', 2010, 'Personalized filtering system', 'A personalized search or filtering system usually suffers from the cold start problem, where the system performs poorly when it has little training data about new users. ', 'Interactive Retrieval, Faceted Feedback, Relevance Feedback, Metadata-based Retrieval', 'OSHUMED dataset, RCV1', 'OSHUMED dataset contains 348,566 medical articles selected from a subset of 270 medical journals covering years from 1987 to 1991. RCV1 (Reuters Corpus Volume 1) dataset contains about 810,000 Reuters news stories published from 1996-08-20 to 1997-08-19. ', 'Faceted document metadata, facet-value pairs', 'Motivated by the commonly used faceted search interface in e-commerce, this paper investigates interactive relevance feedback mechanism based on faceted document metadata. In this mechanism, the system recommends a group of document facet-value pairs, and lets users select relevant ones to restrict the returned documents. We propose four facet-value pair recommendation approaches and two retrieval models that incorporate user feedback on document facets.', 'Evaluated based on user feedback collected through Amazon Mechanical Turk, our experimental results show that the Boolean filtering approach, which is widely used in faceted search in e-commerce, doesn''t work well for text document retrieval, due to the incompleteness (low recall) of metadata assignment in semi-structured text documents.', ''),
(63, 'Query Expansion for Cross Language Information Retrieval Improvement ', 'Benot Gaillard, Jean-Lon Bouraoui, Emilie Guimier de Neef, Malek Boualem  ', 2010, 'Bridging the gap between document and query languages', 'Standard queries, containing 2-3 terms in average, are less and less likely to be sufficient to retrieve all of the relevant documents. Consequently, advanced techniques are necessary to enhance the performances of Information Retrieval (IR) systems, such as Cross Language Information Retrieval (CLIR). Bridging the gap between document and query languages requires the application of machine translation techniques to queries, indices, or both. Translation software can induce linguistic differences between translated data and human language. ', 'Cross-language, Query Translation', 'OPF queries', 'The corpus of queries consists in 51461 different queries (corresponding to 50973 different words2) submitted to OPF (Orange Portal France) for searches on the theme of news, in January 2009.', 'QE with CLIR in a Multilingual Multimedia Information Retrieval', 'The backbone is an Information Retrieval (IR) system based on a search engine and a multilingual module based on statistical machine translation of documents. To this system is added a Query Expansion (QE) module which mainly uses linguistic resources to perform the expansion. In a nutshell, our key idea is to combine QE with CLIR in a Multilingual Multimedia Information Retrieval (MMIR) prototype.', 'More than half of the queries of the complete OPF corpus, including words with spelling mistakes, URLs, etc. are expanded into several choices.  It is worth noticing that the quantity of words provided by QE is unevenly spread across different queries. ', ''),
(64, 'Abstract Feature Extraction for Text Classification', 'Goksel BIRICIK, Banu DIRI, Ahmet Coskun SONMEZ', 2011, 'Efficient model for text classiffication', 'Feature selection and extraction are frequently used solutions to overcome the curse of dimensionality intext classification problems. In vector space, we represent the documents with terms, which is also known as the bag-of-words model. The nature of the bag-of-words approach causes a very high dimensional and sparse feature space. It is hard to build an efficient model for text classiffication in this high dimensional feature space. Due to this problem, dimension reduction has become one of the key problems of textual information processing and retrieval.', 'Abstract feature extraction, high dimensional feature space', 'R2, 20 Newsgroups Mini3 dataset', 'The first dataset is Reuters-215782 and the second is the reduced version of the 20 Newsgroups dataset, which is known as the 20 Newsgroups Mini3 dataset', 'Supervised feature extraction', 'In this paper, we propose a supervised feature extraction method, which produces the extracted features by combining the eï¬€ects of the input features over classes. In the AFE, we combine the in-class term frequencies  with inverse document frequencies and use this scheme to weight the effects of terms on the classes.', 'Comparison and test results show that the AFE scores the highest F1 measure on the Reuters dataset with 96.9%, the 20 Newsgroups dataset with 94.8%, and the ModApte-10 with 96.1%. This means that the AFE achieves a better F1 measure of 3.7% on the Reuters, 19.4% on the 20 Newsgroups, and 3.4% on the ModApte-10 than its nearest following non-AFE method. Looking at the average F1 measures of the classiï¬?ers, we see that the AFEâ€™s score is 9.2% higher on Reuters, 33.0% higher on 20 Newsgroups, and 7.3% higher on ModApte-10 than the next best scored method.', ''),
(65, 'Combining Query Translation Techniques to Improve Cross-language Information Retrieval', 'Benjamin Herbert, Gyoargy Szarvas, and Iryna Gurevych', 2011, 'The rise of multilingual document collections', 'Multilingual information search becomes increasingly important due to the growing amount of online information available in non-English languages and the rise of multilingual document collections. Query translation for CLIR became the most widely used technique to access documents in a different language from the query. As CLIR is less accurate than monolingual IR, the combination of query translation techniques is a promising way to approximate monolingual accuracy.', 'Cross-language, Query Translation', 'CLEF Domain Specific (DS) and Ad Hoc (AH)', 'Document collections. We used two CLIR collections introduced in the CLEF Domain Specific (DS) and Ad Hoc (AH) tracks. They consist of 151,319 German social science and 294,339 newspaper articles and were used in CLEF between 2003-2008 and 2001-2003.', 'Query translation, translation table', 'As a baseline CLIR model, we use query translation by Google Translate.  To exploit this, we mine all redirect and cross-language links to build a translation table which maps concepts to their target language equivalent. To map queries to Wikipedia concepts (titles), we first try to map the whole query, and then gradually proceed with mapping shorter word sequences. ', 'The individual CLIR models perform similar to the results reported in previous works: the Wikipedia model achieves 50-80% of the monolingual result, while Google Translate performs around 90% of the monolingual run. The Wikipedia based concept mapping performs slightly worse than the more complex WP model by [5] but we use just title fields. As regards the combination of the Wikipedia based and the Google translations, we see consistent improvements over the CLIR models using a single translation.', ''),
(66, 'Cross-Language Information Retrieval with Latent Topic Models Trained on a Comparable Corpus', 'Ivan Vulic, Wim De Smet, and Marie-Francine Moens', 2011, 'Availability of translation dictionaries, parallel corpora', 'Translation dictionaries do not exist for every language pair, and they are usually trained on large parallel corpora, where each document has an exact translation in the other language, or are hand-built. Parallel corpora are not available for each language pair. In contrast, comparable corpora in which documents in the source and the target language contain similar content, are usually available in abundance. In this paper we address the question whether suitable cross-language retrieval models can be built based on the interlingual topic representations learned from comparable corpora.', 'Cross-language, Topic Models, Comparable Corpus', 'Europarl corpus, Wikipedia dumps3, CLEF 2001 - 2003 CLIR campaigns: LA Times 1994, LA Times 1994 and Glasgow Herald 1995 in English, NRC Handelsblad 1994 - 1995, Algemeen Dagblad 1994 - 1995 in Dutch', 'Europarl corpus, extracted from proceedings of the European Parliament and consisting of 6, 206 parallel documents in English and Dutch.  Wikipedia dumps3 consists of paired documents in English and Dutch.', 'Latent Dirichlet Allocation (BiLDA)', 'We accomplish this goal by means of a cross-language generative model, i.e., bilingual Latent Dirichlet Allocation (BiLDA), trained on a comparable corpus such as one composed of Wikipedia articles. The resulting probabilistic translation model is incorporated in a statistical language model for information retrieval. The topic model we use is a bilingual extension of a standard LDA model, called bilingual LDA (BiLDA).', 'The LDA-only model seems to be too coarse to be used as the only component of an IR model (e.g., due to its limited number of topics, words in queries unobserved during training). However, the combination of the LDA-only and the simple unigram model, which allows retrieving relevant documents based on shared words across the languages (e.g. personal names), leads to much better scores which are competitive even with models which utilize cross-lingual dictionaries or machine translation systems. ', ''),
(67, 'TopicTiling: A Text Segmentation Algorithm based on LDA', 'Martin Riedl and Chris Biemann', 2012, 'Text Segmentation', 'The task tackled in this paper is Text Segmentation (TS), which is to be understood as the segmentation of texts into topically similar units. The challenge for a text segmentation algorithm is to find the subtopical structure of a text.', 'Text Segmentation', 'Cross-linked Information Resources (CLIR)', 'CLIR test collections for three languages: TREC 2002 English-Arabic CLIR, NTCIR-8 English-Chinese Advanced Cross-Lingual Information Access (ACLIA), and CLEF 2006 English-French CLIR.', 'TextTiling, Latent Dirichlet Allocation', 'This algorithm is based on the well-known TextTiling algorithm, and segments documents using the Latent Dirichlet Allocation (LDA) topic model. TopicTiling uses topic IDs, obtained by the LDA inference method, instead of words. We denote this most probable topic ID as the mode (most frequent across all inference steps) of the topic assignment. These IDs are used to calculate the cosine similarity between two adjacent blocks of sentences, represented as two vectors, containing the frequency of each topic ID.', 'We show that using the mode topic ID assigned during the inference method of LDA, used to annotate unseen documents, improves performance by stabilizing the obtained topics. We show significant improvements over state of the art segmentation algorithms on two standard datasets. As an additional benefit, TopicTiling performs the segmentation in lin- ear time and thus is computationally less expensive than other LDA-based segmentation methods.', ''),
(68, 'First Order Statistics Based Feature Selection: A Diverse and Powerful Family of Feature Seleciton Techniques', 'Taghi Khoshgoftaar, David Dittman, Randall Wald, and Alireza Fazelpour', 2012, 'Reduction of the dimensionality of a dataset', 'One of the most prevalent problems in DNA microarray datasets is the large degree of high dimensionality that is inherent in the data. Feature selection refers to a diverse series of techniques from the field of data mining designed for the reduction of the dimensionality of a dataset. However, there are a number of feature selection techniques which are computationally infeasible due to the severe level of high dimensionality found in DNA microarray datasets.', 'DNA datasets', 'Cross-linked Information Resources (CLIR)', 'CLIR test collections for three languages: TREC 2002 English-Arabic CLIR, NTCIR-8 English-Chinese Advanced Cross-Lingual Information Access (ACLIA), and CLEF 2006 English-French CLIR.', 'First Order Statistics (FOS)', 'In order to examine the properties of these seven techniques we performed a series of similarity and classification experiments on eleven DNA microarray datasets. This paper presents a set of seven univariate feature selection techniques which we have combined into a family of techniques we name First Order Statistics (FOS) based feature selection.', 'In terms of similarity, we find that each ranker in the FOS family of techniques will create diverse feature subsets when compared to other members of the family', ''),
(69, 'Accurate Query Translation for Japanese-English Cross-language Information Retrieval', 'Vitaly Klyuev and Yannis Haralambous', 2012, 'Queries translation', 'Cross-Language Information Retrieval (CLIR) can  be used to retrieve documents in one language in response to a query given in another. The usual approach consists of two steps: 1) translation of the user query into the target language and then 2) retrieval of documents in this language by using a conventional mono-lingual information retrieval system. In this paper, we propose a novel approach to translate queries for a Japanese-English CLIR task.', 'Query Translation', 'Cross-linked Information Resources (CLIR)', 'CLIR test collections for three languages: TREC 2002 English-Arabic CLIR, NTCIR-8 English-Chinese Advanced Cross-Lingual Information Access (ACLIA), and CLEF 2006 English-French CLIR.', 'EWC semantic relatedness, Wikipedia-based Explicit Semantic Analysis measure', 'To get all possible English senses for every Japanese term, the online dictionary SPACEALC is utilized. The EWC semantic relatedness measure is used to select the most related meanings for the results of translation. This measure combines the Wikipedia-based Explicit Semantic Analysis measure, the WordNet path measure and the mixed collocation index.', 'Retrieval performance with queries generated utilizing Mecab was very low. Our preliminary experiments showed the superiority of the longest much technique applying SPACEALC over Mecab: Segmentation of Japanese texts is much more accurate.', ''),
(70, 'Query Translation Using Concepts Similarity based on Quran Ontology for Cross-language Information Retrieval', 'Zulaini Yahya, Muhamad Taufik Abdullah, Azreen Azman and Rabiah Abdul Kadir ', 2012, 'Multi meaning words', 'In Cross-Language Information Retrieval (CLIR) process, the translation effects have a direct impact on the accuracy of follow-up retrieval results. In dictionary-based approach, we are dealing with the words that have more than one meaning which can decrease the retrieval performance if the query translation return an incorrect translations.', 'Multi meaning query', 'Cross-linked Information Resources (CLIR)', 'CLIR test collections for three languages: TREC 2002 English-Arabic CLIR, NTCIR-8 English-Chinese Advanced Cross-Lingual Information Access (ACLIA), and CLEF 2006 English-French CLIR.', 'Domain ontology using Quran concepts', 'In this study we proposed a Cross-Language Information Retrieval (CLIR) method based on domain ontology using Quran concepts for disambiguating translation of the query and to improve the dictionary-based query translation.', 'The experimental result shows that our proposed method brings significant improvement in retrieval accuracy for English document collections, but deficient for Malay document collections.', ''),
(71, 'Combining Statistical Translation Techniques for Cross-Language Information Retrieval', 'Ferhan Ture1, Jimmy Lin, Douglas W. Oard', 2012, 'The use of old model of statistical machine translation systems', 'Cross-language information retrieval today is dominated by techniques that rely principally on context-independent token-to-token mappings despite the fact that state-of-the-art statistical machine translation systems now have far richer translation models available in their internal representations.', 'Machine Translation', 'Cross-linked Information Resources (CLIR)', 'CLIR test collections for three languages: TREC 2002 English-Arabic CLIR, NTCIR-8 English-Chinese Advanced Cross-Lingual Information Access (ACLIA), and CLEF 2006 English-French CLIR.', 'Three types of statistical translation models', 'This paper explores combination-of-evidence techniques using three types of statistical translation models: context-independent token translation, token translation using phrase-dependent contexts, and token translation using sentence-dependent contexts. Context-independent translation is performed using statistically-aligned tokens in parallel text, phrase-dependent translation is performed using aligned statistical phrases, and sentence-dependent translation is performed using those same aligned phrases together with an n-gram language model.', 'Experiments on retrieval of Arabic, Chinese, and French documents using English queries show that no one technique is optimal for all queries, but that statistically significant improvements in mean average precision over strong baselines can be achieved by combining translation evidence from all three techniques.', ''),
(72, 'Classifier Chains for Multi-label Classification', 'Jesse Read, Bernhard Pfahringer, Geoff Holmes, Eibe Frank	', 2009, 'Multi-label Classification', 'The widely known binary relevance method for multi-label The widely known binary relevance method for multi-label\r\n		classification, which considers each label as an independent binary problem', 'Classifier Chains ', 'Automatic Content Extraction (ACE) corpus', 'Automatic Content Extraction (ACE) corpus available from LDC (LDC2003T11)', 'novel chaining method that can model label correlations', 'exemplify this with a novel chaining method that can model label correlations while\r\n		maintaining acceptable computational complexity', 'binary relevance-based methods have much to offer, especially in terms of scalability to large datasets', ''),
(151, 'On the Role of Lexical Features in Sequence Labeling', 'Yoav Goldbergâˆ— and Michael Elhadad', 2009, 'Sequence Labeling', 'Common NLP tasks, such as Named Entity Recognition and Chunking, involve the identification of spans of words belonging to the same phrase', 'Lexical Features', 'Dutch data set', 'Dutch data set from the CoNLL 2002 shared task (Tjong Kim Sang, 2002)', 'On the Role of Lexical Features', 'use the technique of SVM anchoring to demonstrate that lexical features extracted\r\n		from a training corpus are not necessary to obtain state of the art results on tasks such\r\n		as Named Entity Recognition and Chunking', 'that exact word forms aren''t necessary for accurate classification', ''),
(152, 'Combining Lexical and Grammatical Features to Improve Readability Measures for First and Second Language Texts', 'Michael J. Heilman, Kevyn Collins-Thompson, Jamie Callan, Maxine Eskenazi ', 2007, 'Improve Readability Measures for First and Second Language Texts', 'Combining Lexical and Grammatical Features to Improve Readability Measures for First and Second Language Texts', 'Combining Lexical', 'grammatical features'' role in English', ' The first corpus was from a set of texts \r\ngathered from the Web for a prior evaluation of the language modeling approach.  The first corpus was from a set of texts gathered from the Web for a prior evaluation of the language modeling approach.', 'Combining Lexical and Grammatical Features', 'The statistical model used for this study is based on a variation of the multinomial Naive Bayes classifier.', 'grammatical features can play a role in predicting reading difficulty levels for both first and second language texts in English', ''),
(153, 'Statistical Section Segmentation in Free-Text Clinical Records', 'Michael Tepper1, Daniel Capurro2, Fei Xia1,2, Lucy Vanderwende3,2, Meliha Yetisgen-Yildiz2,1', 2001, 'an approach to automatic section segmentation of clinical records', 'an approach to automatic section segmentation of clinical records such as hospital discharge summaries and radiology reports, along with section classification into pre-defined section categories.', 'Document segmentation, Clinical NLP, Text classification', 'three datasets composed of discharge summeries and radiology reports', 'three datasets composed of discharge summaries and radiology reports to develop our statistical section segmenter and test its performance', 'classify each line in a document', 'classify each line in a document to indicate its membership to a section', 'For Datasets 1 and 2, the performance results show that the two-step approach (Experiments 2, 3, and 4) outper forms the one-step approach (Experiment 1). For Dataset 3, the two-step approach slightly decreased the performance; however, the\r\ndifferences are too small to draw strong conclusions', ''),
(154, 'Shallow Semantics for Relation Extraction', 'Sanda Harabagiu, Cosmin Adrian Bejan and Paul MoraË˜rescu', 2005, 'Relation Extraction	', 'extracting meaningful relations from unstructured natural language sources', 'Shallow Semantics', 'Automatic Content Extraction (ACE) corpus', 'Automatic Content Extraction (ACE) corpus available from LDC (LDC2003T11)', 'Shallow Semantics', 'The method is based on information made available by shallow semantic parsers.\r\nSemantic information was used (1) to enhance a dependency tree kernel; and (2) to build semantic dependency structures used for enhanced relation extraction for several semantic classifiers', 'frame semantics produce an enhancement of\r\n53.24% over previous state-of-art results in relation extraction. Furthermore, they show that semantic representations such as frames or predicate-argument structures have a wider impact on classification performance than the classification technique', ''),
(155, 'Pattern Learning for Relation Extraction with a Hierarchical Topic Model', 'Enrique Alfonseca Katja Filippova Jean-Yves Delort', 2012, 'Relation Extraction with a Hierarchical Topic Model', 'The main contribution of this work is presenting a variant of distance supervision for relation extraction \r\nwhere we do not use heuristics in the selection of the training data', 'Relation Extraction', 'English news articles corpus', 'Text corpus used contains 33 million English news articles', 'Pattern Learning', 'Similar to other distant supervision methods, our approach takes as input an existing knowledge base containing entities and relations, and a textual corpus. In this work it is not necessary for the corpus to be related to the knowledge base. In what follows we assume that all the relations studied are binary and hold between exactly two entities in the knowledge base. We also assume a dependency parser is available, and that the entities have been automatically disambiguated using the knowledge base as sense inventory.', 'The MLE base lines (in red with syntactic patterns and green with intertext) perform consistently worse than the models learned using the topic models (in pink and blue). The difference in precision, aggregated across all relations, is statistically significant at 95% confidence for most of the thresholds.', ''),
(156, ' Class Imbalance Problem in Data Mining: Review ', 'Mr.Rushi Longadge, Ms. Snehlata S. Dongre, Dr. Latesh Malik ', 2013, 'Class Imbalance', 'Classification of data becomes difficult because of unbounded size and imbalance nature of data. Class imbalance problem \r\nbecome greatest issue in data mining. Imbalance problem occur where one of the two classes having more sample than other \r\nclasses', 'Class imbalance', 'Imbalanced datasets', 'Imbalanced datasets', 'The algorithmic approach, data-\r\npreprocessing approach and feature selection approach', 'There are different methods available \r\nfor classification of imbalance data set which is divided into three main categories, the algorithmic approach, data-\r\npreprocessing approach and feature selection approach.', 'suggests that applying two or more technique i.e. hybrid \r\napproach gives better solution for class imbalance \r\nproblem', ''),
(157, 'A Framework of Feature Selection Methods for Text Categorization', 'Shoushan Li1  Rui Xia2  Chengqing Zong2  Chu-Ren Huang1', 2009, ' Text Categorization', 'In text categorization, feature selection (FS) is a strategy that aims at making text classifiers more efficient and accurate', 'Text categorization', 'Automatic Content Extraction (ACE) corpus', 'Automatic Content Extraction (ACE) corpus available from LDC (LDC2003T11)', 'A Framework of Feature Selection Methods', 'feature selection (FS) is a strategy that aims at making text classifiers more efficient and accurate.', 'this new method is robust across different tasks and numbers of selected features', ''),
(158, 'Feature Selection and Feature Extract ion for Text Categorization', 'David D. Lewis ', 2008, ' Text Categorization', 'examine the effect of feature set size on text categorization effectiveness', 'Text Categorization', 'Five test data sets(CACM, MED, EXC, PEO and TOP)', 'Two data sets,CACM and MED, are extracted from the CACM and MEDLINE abstracts, respectively, which are included in the Classic database. Additional three data sets, EXC, PEO and TOP,are from the EXCHANGES, PEOPLE and TOPICS category setsof the Reuters-21578 Distribution 1.0', 'Feature Selection and Feature Extraction', 'If a syntactic parse of text is available, then features can be defined by the presence of two or more words in \r\nparticular syntactic relationships. We call such a feature a syntactic indexing phrase.', 'Good categorization performance was achieved using a statistical classifier and a proportional assignment strategy', ''),
(159, 'High-Performing Feature Selection for Text Classification', 'Monica Rogati,Yiming Yang', 2008, 'Text Classification', 'Text Classification', 'Text Classification', 'Five test data sets(CACM, MED, EXC, PEO and TOP)', 'Two data sets,CACM and MED, are extracted from the CACM and MEDLINE abstracts, respectively, which are included in the Classic database. Additional three data sets, EXC, PEO and TOP,are from the EXCHANGES, PEOPLE and TOPICS category setsof the Reuters-21578 Distribution 1.0', 'High-Performing Feature Selection', 'Selected four high-performing classifiers for the feature selection experiments:\r\n1. K-Nearest Neighbors (local implementation)\r\n2. Naive Bayes (Rainbow, [7])\r\n3. Rocchio (local implementation)\r\n4. Support Vector Machines (SVMLight, [3])', 'The results obtained using only 3% of the available features are among the best reported, including results obtained with the full feature set.', '');
INSERT INTO `data_penelitian` (`id`, `judul`, `peneliti`, `tahun_publikasi`, `masalah`, `deskripsi_masalah`, `keyword`, `domain_data`, `deskripsi_domain_data`, `metode`, `deskripsi_metode`, `hasil`, `creater`) VALUES
(160, 'Summarization as Feature Selection for Text\r\nCategorization', 'Aleksander Kotcz Vidya Prabakarmurthi Jugal  Kalita', 2008, 'Text Categorization', 'the problem addressed of evaluating the effectiveness of summarization techniques for the task of document categorization.', 'Text Categorizaton', 'Five test data sets(CACM, MED, EXC, PEO and TOP)', 'Two data sets,CACM and MED, are extracted from the CACM and MEDLINE abstracts, respectively, which are included in the Classic database. Additional three data sets, EXC, PEO and TOP,are from the EXCHANGES, PEOPLE and TOPICS category setsof the Reuters-21578 Distribution 1.0', 'Summarization as Feature Selection', 'To quantify the arguments advanced in this paper, we considered a number of simple extraction-based techniques the details are given in list below. Similar heuristics-based techniques have been used for example in [16]  [5]  [17].  In all\r\ncases, a word occurring at least 3 times in the body of a document was considered a keyword, while a word occurring at\r\nleast once in the title of a story was considered a title word.', 'A framework proposed for evaluating summarization methods in the context of their utility as feature selectors in automatic text categorization. Our approach is well suited for classifiers utilizing binary feature vectors, where a\r\nfeature corresponds to the presence or absence of a word in a document.', ''),
(161, 'Identifying Sections in Scientific Abstracts using Conditional Random Fields', 'Kenji Hirohata,Sophia Ananiadou, Mitsuru Ishizuka', 2007, 'Identifying Sections in Scientific Abstracts', 'The prior knowledge about\r\nthe rhetorical structure of scientific abstracts is useful for various text-mining tasks such as information extraction, information retrieval, and automatic summarization', ' Scientific Abstracts, Conditional Random Fields', ' Scientific Abstracts', 'Given an unstructured abstract without section labels indicated by boldface\r\ntype, the proposed method annotates section labels of each sentence', 'Conditional Random Fields', 'Formalizing the categorization task as a sequential labeling problem, we employ Conditional Random Fields (CRFs) to annotate section labels into abstract sentences.', 'The proposed method outperformed the previous approaches, achieving 95.5% per-sentence\r\naccuracy and 68.8% per-abstract accuracy.\r\nCONCLUSION: The experimental results\r\nshowed that CRFs could model the rhetor-\r\nical structure of abstracts more suitably', ''),
(162, 'Section Classification in Clinical Notes using Supervised Hidden Markov Model', 'Ying Li, Sharon Lipsky Gorman, Noaomie Elhadad', 2010, 'Section Classification in Clinical', 'As more and more information is available in the Electronic Health Record in the form of free-text narrative, there is a\r\nneed for automated tools, which can process and understand such texts', 'Clinical Natural Language Processing, Discourse Analysis, Section Labeling, Hidden Markov Model', 'Clinical notes', ' The test set contains 2,088 clinical notes,corresponding to 11,706 text spans', 'Supervised Hidden Markov Model', ' Our method relies on a Hidden Markov Model (HMM) trained on a corpus\r\nof 9,679 clinical notes from NewYork-Presbyterian Hospital.', 'An hyphotesis that section sequences can improve the accuracy of section label classification', ''),
(163, 'Document Segmentation And Region Classification Using Multilayer Perceptron  ', 'Priyadharshini N1, Vijaya MS2', 2013, 'Document Segmentation And Region Classification', 'Document segmentation is a process of splitting the document into distinct regions', 'Document analysis, Information retrieval, Classification, \r\nFeature extraction, Document segmentation', 'various journals ', 'The document image classification model is generated by implementing supervised learning algorithm. The documents \r\nused for creating the dataset are collected from various journals and here 15 document pages have been used.', 'Multilayer Perceptron  ', 'Bottom up approach \r\nto segment the document image into homogenous regions based \r\non geometric structure', 'classification accuracy produced by multilayer perceptron is higher than naive bayes and J48', ''),
(164, 'Causal Relation Extraction', 'Eduardo Blanco, Nuria Castell, Dan Moldovan', 2005, 'The automatic detection and extraction of Semantic Relations', 'The automatic detection and extraction of Semantic Relations is a crucial step to improve the performance of several\r\nNatural Language Processing applications.', 'Causal Relation', 'TREC5 corpus', '1270 sentences from the TREC5\r\ncorpus into encoding or not encoding causation; 170 intrasentencial causations were found', 'A supervised method', 'a supervised method for the detection and extraction of Causal Relations from open domain text', 'a system proposed for the detection of marked and\r\nexplicit causations between a verb phrase and a subordinate clause which yields a high performance. The system\r\nis relatively simple and is able to detect causations from\r\nopen domain text', ''),
(165, 'Learning context-free grammars to extract relations from text', 'Georgios Petasis and Vangelis Karkaletsis and Georgios Paliouras and Constantine D. Spyropoulos ', 2008, 'Relation extraction', 'Relation extraction is the task of identifying the relations that hold \r\nbetween interesting entities in text data.', ' context-free grammars, relation extraction', 'BOEMIE research project ', 'annotated corpus from the \r\nBOEMIE research project was used', 'a novel relation extraction method', 'a novel relation extraction \r\nmethod, based on grammatical inference. Following a semisupervised learning approach, the text that connects named entities in an annotated corpus is used to infer a context free grammar.', 'The proposed approach performs comparable to the state of the art, \r\nwhile exhibiting a bias towards precision, which is a sign of \r\nconservative generalisation', ''),
(166, 'RelEx-Relation extraction using dependency parse trees', 'Katrin Fundeli, Robert Kuaffner and Ralf Zimmer', 2007, 'Relation extraction', 'The discovery of regulatory pathways, signal cascades,\r\nmetabolic processes or diseasemodels requires knowledge on individual relations like e.g. physical or regulatory interactions between\r\ngenes and proteins. Most interactions mentioned in the free text of\r\nbiomedical publications are not yet contained in structured databases.', 'Relation extraction, Parse Trees', 'RelEx', 'RelEx on a comprehensive set of one million\r\nMEDLINE abstracts dealing with gene and protein relations and\r\nextracted ~150000 relations with an estimated perfomance of both\r\n80% precision and 80% recall.', 'Dependency parse trees', 'It uses a small set of simple rules, building upon publicly available tools\r\napplied for part-of-speech-tagging, noun-phrase-chunking and\r\ndependency.', 'RelEx on a comprehensive set of one million\r\nMEDLINE abstracts dealing with gene and protein relations and\r\nextracted ~150000 relations with an estimated perfomance of both\r\n80% precision and 80% recall.', ''),
(167, 'Multi-pattern wrappers for relation extraction from the Web', 'Benjamin Habegger and Mohamed Quafafou', 2002, 'Information extraction', 'The extraction of information from the content of these sources is a challenging problem and a hard task since they are het-\r\nerogeneous and dynamic.', 'Relation extraction, Multi-pattern wrappers', 'search-engines, product catalogs and bibliography listings', 'Dataset of search-engines, product catalogs and bibliography listings', 'A new method for extracting wrappers and relations', 'A new method for extracting wrappers and relations from the web using both page encoding and context generalization.Multiple patterns\r\nare then extracted considering the occurrences of the input instances\r\nin the data source', 'All the built wrappers correctly\r\nextract the full page with a couple of instances', ''),
(168, 'Multi-Document Summarisation Using Generic Relation Extraction', 'Ben Hachey', 2009, 'shallow features often break down', 'The problem is that these shallow features often break down where underlying linguistic content needs to be compared rather than just surface structure.', 'Multi-docs Summary', 'Automatic Content Extraction (ACE) corpus', 'Automatic Content Extraction (ACE) corpus available from LDC (LDC2003T11)', 'Generic Relation Extraction (GRE)', 'A novel representation is introduced based on generic relation extraction (GRE), which\r\naims to build systems for relation iden-\r\ntification and characterisation that can be\r\ntransferred across domains and tasks with-\r\nout modification of model parameters', 'Performance for the relation representation is significantly better than anon-trivial tf*idf baseline across the range of summary lengths explored. Performance is also at\r\nleast as good as a comparable but less general representation based on event extraction. Correlation analysis suggests that different representations are\r\ncomplementary due to the fact that they perform well on different document sets.', ''),
(169, 'Dependency Tree Kernels for Relation Extraction', 'Aron Culotta,Jeffrey Sorensen ', 2004, 'Relation Extraction', 'tree kernels to estimate the similarity between the dependency trees of sentences. ', 'Tree kernels, Relation Extraction', 'Automatic Content Extraction (ACE) corpus', 'Relations from the Automatic Content Extraction (ACE) corpus provided by the National Institute for Standards and Technology (NIST). T', 'Using Tree Kernels', 'Using this kernel within a Support Vector Machine, we detect and classify relations\r\nbetween entities in the Automatic Content Extraction (ACE) corpus of news articles', 'using a dependency tree kernel for relation extraction provides a vast improvement over a bag-of-words kernel. While the dependency tree kernel appears to perform well at\r\nthe task of classifying relations, recall is still relatively low', ''),
(170, 'Kernel-based Relation Extraction from Investigative Data', 'Cristina Giannone,Roberto Basili,Chiara Del Vescovo,Paolo Naggar,Alessandro Moschitti  ', 2009, 'Kernel-based Relation Extraction', ' The recognition and storage of complex relations among subjects mentioned in these sources is a very difficult and time consuming task, ultimately based on pools\r\nof experts. ', 'Kernel-based RE', 'Automatic Content Extraction (ACE) corpus', 'Automatic Content Extraction (ACE) corpus available from LDC (LDC2003T11)', 'SVMs', 'SVMs here are employed to produce a set\r\nof possible interpretations for domain relevant concepts', 'The empirical investigation presented here shows that accurate results, comparable to the expert teams, can be achieved, and parametrization allows\r\nto fine tune the system behavior for fitting the specific domain requirements.', ''),
(171, 'A Logic-Based Approach to Relation Extraction from Texts', 'Tamas Horvath, Gerhard Paass, Frank Reichartz, and Stefan Wrobel', 2006, ' Relation Extraction', 'In recent years, text mining has moved far beyond the classical problem of text classification with an increased interest in more sophisticated processing of large text corpora, such as, for example, eval-\r\nuations of complex queries. ', ' Logic-Based Approach', 'Benchmark dataset ACE', 'Benchmark dataset ACE 20034', 'Logic-Based Approach', 'This problem becomes a typical application of learning logic programs by considering the dependency trees of sentences as relational structures and examples of the target relation as ground atoms of a target predicate', 'Empirical results on a popular\r\nbenchmark dataset indicate that the performance of our method is comparable\r\nwith state-of-the-art methods', ''),
(172, 'Simple Algorithms for Complex Relation Extraction with Applications to Biomedical IE', 'Ryan McDonald Fernando Pereira1 Seth Kulick,Scott Winters Yang Jin Pete White', 2005, ' Complex Relation Extraction', 'A complex relation is any n-ary relation\r\nin which some of the arguments may be\r\nbe unspecified.', ' Biomedical IE,Complex Relation Extraction ', 'MEDLINE ', 'data set consists of 447 abstracts selected\r\nfrom MEDLINE as being relevant to populating a database with facts of the form: gene X with variation event Y is associated with malignancy Z.', 'A simple method for extracting complex relations', ' This method works by first fac-\r\ntoring all complex relations into a set of binary relations. A classifier is then trained in the standard\r\nmanner to recognize all pairs of related entities. Finally a graph is constructed from the output of this\r\nclassifier and the complex relations are determined\r\nfrom the cliques of this graph.', 'The system based on binary factorization not only is more efficient then naively enumerating all instances, but significantly outperforms it as well', ''),
(173, 'Extracting Semantic Relations\r\nfor Mining of Social Data', 'Shinichi Nagano, Masumi Inaba, and Takahiro Kawamura', 2014, 'semantic relation extraction', 'the issue of semantic relation extraction from documents on the Social Web', 'Extracting Semantic Relations, Social Data', 'web documents', 'a set of three hundred Web documents, which\r\nare blog posts mentioned on food products', 'Bootstrapping and clustering', 'A set of nouns are iteratively extracted from documents in a bootstrapping manner, and then a semantic relation\r\nbetween a noun pair is identified by a clustering procedure. The main\r\nfeature is exploitation of the co-occurrence of a verb and a noun in a\r\nsentence, considering that a verb plays an important role in expressing\r\nthe meaning of a sentence', 'precision value is 0.806 in case that the number of seed pairs is 10, and it is within a range of 0.2 to 0.4 in other cases. On the other hand, a\r\nnumber of false negative pairs resulted in low recall.', ''),
(174, 'Distant supervision for relation extraction without labeled data', 'Mike Mintz, Steven Bills, Rion Snow, Dan Jurafsky', 2009, ' Relation extraction without labeled data', 'Modern models of relation extraction for tasks like\r\nACE are based on supervised learning of relations\r\nfrom small hand-labeled corpora. We investigate an\r\nalternative paradigm that does not require labeled\r\ncorpora, avoiding the domain dependence of ACE-\r\nstyle algorithms, and allowing the use of corpora\r\nof any size', 'Distant supervision', 'Automatic Content Extraction (ACE) corpus', 'Automatic Content Extraction (ACE) corpus available from LDC (LDC2003T11)', 'Distant supervision', 'For each pair of enti-\r\nties that appears in some Freebase relation, we find\r\nall sentences containing those entities in a large un-\r\nlabeled corpus and extract textual features to train\r\na relation classifier. ', 'Our model is able to extract 10,000 instances of 102 relations at a precision of 67.6%. We also analyze\r\nfeature performance, showing that syntactic parse\r\nfeatures are particularly helpful for relations that are\r\nambiguous or lexically distant in their expression.', ''),
(175, 'Unsupervised Relation Extraction by Massive Clustering', 'Edgar Gonza`lez, Jordi Turmo', 2009, ' Information Extraction', 'The goal of Information Extraction is to automatically generate structured pieces of information from the\r\nrelevant information contained in text documents.', 'Unsupervised RE', 'Automatic Content Extraction (ACE) corpus', 'Automatic Content Extraction (ACE) corpus available from LDC (LDC2003T11)', 'As learning corpus we used the year 2000 subset of the\\r\\nAssociated Press section of the AQUAINT Corpus.', 'an unsupervised approach to\r\nlearning for Relation Detection, based on the use of massive\r\nclustering ensembles.', 'The results obtained on the ACE Relation Mention Detection\r\ntask outperform in terms of F1 score by 5 points the state of the\r\nart of unsupervised techniques for this evaluation framework,\r\nin addition to being simpler and more flexible.', '');

-- --------------------------------------------------------

--
-- Table structure for table `metadata_penelitian`
--

CREATE TABLE IF NOT EXISTS `metadata_penelitian` (
`id` int(11) NOT NULL,
  `col_name` text NOT NULL,
  `deskripsi` text NOT NULL,
  `flag` tinyint(4) NOT NULL
) ENGINE=InnoDB  DEFAULT CHARSET=latin1 AUTO_INCREMENT=20 ;

--
-- Dumping data for table `metadata_penelitian`
--

INSERT INTO `metadata_penelitian` (`id`, `col_name`, `deskripsi`, `flag`) VALUES
(1, 'judul', 'Judul', 1),
(2, 'tahun_publikasi', 'Tahun Publikasi', 1),
(5, 'domain_data', 'Domain Data', 1),
(8, 'metode', 'Metode', 1),
(9, 'hasil', 'Hasil', 0),
(13, 'masalah', 'Masalah', 1),
(14, 'citation', 'Citation', 0),
(15, 'peneliti', 'Peneliti', 1),
(16, 'keyword', 'Keyword', 0),
(17, 'deskripsi_domain_data', 'Deskripsi Domain Data', 0),
(18, 'deskripsi_metode', 'Deskripsi Metode', 0),
(19, 'deskripsi_masalah', 'Deskripsi Masalah', 0);

-- --------------------------------------------------------

--
-- Table structure for table `metadata_relasi`
--

CREATE TABLE IF NOT EXISTS `metadata_relasi` (
`id` int(11) NOT NULL,
  `deskripsi` text NOT NULL,
  `keterangan` text NOT NULL
) ENGINE=InnoDB  DEFAULT CHARSET=latin1 AUTO_INCREMENT=3 ;

--
-- Dumping data for table `metadata_relasi`
--

INSERT INTO `metadata_relasi` (`id`, `deskripsi`, `keterangan`) VALUES
(1, 'Citation', 'Relasi menunjukkan paper satu merujuk ke paper lain'),
(2, 'Improvement', 'Relasi peningkatan hasil satu paper ke paper lain\r\n');

-- --------------------------------------------------------

--
-- Table structure for table `relasi`
--

CREATE TABLE IF NOT EXISTS `relasi` (
`id` int(11) NOT NULL,
  `id_relasi` int(11) NOT NULL,
  `id_paper_1` int(11) NOT NULL,
  `id_paper_2` int(11) NOT NULL,
  `creater` text NOT NULL
) ENGINE=InnoDB  DEFAULT CHARSET=latin1 AUTO_INCREMENT=14 ;

--
-- Dumping data for table `relasi`
--

INSERT INTO `relasi` (`id`, `id_relasi`, `id_paper_1`, `id_paper_2`, `creater`) VALUES
(1, 2, 12, 11, '7'),
(2, 1, 1, 2, ''),
(3, 2, 12, 15, ''),
(4, 2, 1, 2, ''),
(5, 1, 2, 1, ''),
(6, 1, 2, 6, ''),
(7, 1, 2, 20, ''),
(9, 1, 10, 11, ''),
(10, 1, 12, 4, ''),
(11, 1, 11, 10, '1'),
(12, 1, 2, 11, '1'),
(13, 1, 8, 12, '1');

-- --------------------------------------------------------

--
-- Table structure for table `saved_map`
--

CREATE TABLE IF NOT EXISTS `saved_map` (
`id` int(11) NOT NULL,
  `id_user` int(11) NOT NULL,
  `id_paper` text NOT NULL,
  `parameter_x` text NOT NULL,
  `parameter_y` text NOT NULL,
  `parameter_relation` text NOT NULL,
  `map_name` text NOT NULL
) ENGINE=InnoDB  DEFAULT CHARSET=latin1 AUTO_INCREMENT=19 ;

--
-- Dumping data for table `saved_map`
--

INSERT INTO `saved_map` (`id`, `id_user`, `id_paper`, `parameter_x`, `parameter_y`, `parameter_relation`, `map_name`) VALUES
(1, 7, '2,32,8,11,12,10', 'Domain Data', 'Tahun Publikasi', 'Citation', 'Text Clustering'),
(16, 7, '8,10,11,12,13,14,15,16,17,18,19', 'Domain Data', 'Tahun Publikasi', 'Citation', 'Text Categorization'),
(17, 7, '8,10,11,12,13,14,15,16,17,18,19,3,5,6', 'Peneliti', 'Tahun Publikasi', 'Citation', 'Summarization'),
(18, 7, '2,8,10,11,12,32,6', 'Domain Data', 'Tahun Publikasi', 'Citation', 'Word Similarity');

-- --------------------------------------------------------

--
-- Table structure for table `tbl_profiles`
--

CREATE TABLE IF NOT EXISTS `tbl_profiles` (
  `user_id` int(11) NOT NULL,
  `lastname` varchar(50) NOT NULL DEFAULT '',
  `firstname` varchar(50) NOT NULL DEFAULT '',
  `birthday` date NOT NULL DEFAULT '0000-00-00'
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

--
-- Dumping data for table `tbl_profiles`
--

INSERT INTO `tbl_profiles` (`user_id`, `lastname`, `firstname`, `birthday`) VALUES
(1, 'Admin', 'Administrator', '0000-00-00'),
(2, 'Demo', 'Demo', '0000-00-00'),
(3, 'anti', 'yuli', '2014-09-24'),
(4, 'anti', 'yuli', '2014-09-29'),
(5, 'oenang', 'yulianti', '2014-10-07'),
(6, 'oenang', 'yulianti', '2014-10-05'),
(7, 'oenang', 'yulianti', '2014-10-05'),
(8, 'oenang', 'yulianti', '2014-10-09');

-- --------------------------------------------------------

--
-- Table structure for table `tbl_profiles_fields`
--

CREATE TABLE IF NOT EXISTS `tbl_profiles_fields` (
`id` int(10) NOT NULL,
  `varname` varchar(50) NOT NULL,
  `title` varchar(255) NOT NULL,
  `field_type` varchar(50) NOT NULL,
  `field_size` int(3) NOT NULL DEFAULT '0',
  `field_size_min` int(3) NOT NULL DEFAULT '0',
  `required` int(1) NOT NULL DEFAULT '0',
  `match` varchar(255) NOT NULL DEFAULT '',
  `range` varchar(255) NOT NULL DEFAULT '',
  `error_message` varchar(255) NOT NULL DEFAULT '',
  `other_validator` varchar(5000) NOT NULL DEFAULT '',
  `default` varchar(255) NOT NULL DEFAULT '',
  `widget` varchar(255) NOT NULL DEFAULT '',
  `widgetparams` varchar(5000) NOT NULL DEFAULT '',
  `position` int(3) NOT NULL DEFAULT '0',
  `visible` int(1) NOT NULL DEFAULT '0'
) ENGINE=InnoDB  DEFAULT CHARSET=utf8 AUTO_INCREMENT=4 ;

--
-- Dumping data for table `tbl_profiles_fields`
--

INSERT INTO `tbl_profiles_fields` (`id`, `varname`, `title`, `field_type`, `field_size`, `field_size_min`, `required`, `match`, `range`, `error_message`, `other_validator`, `default`, `widget`, `widgetparams`, `position`, `visible`) VALUES
(1, 'lastname', 'Last Name', 'VARCHAR', 50, 3, 1, '', '', 'Incorrect Last Name (length between 3 and 50 characters).', '', '', '', '', 1, 3),
(2, 'firstname', 'First Name', 'VARCHAR', 50, 3, 1, '', '', 'Incorrect First Name (length between 3 and 50 characters).', '', '', '', '', 0, 3),
(3, 'birthday', 'Birthday', 'DATE', 0, 0, 2, '', '', '', '', '0000-00-00', 'UWjuidate', '{"ui-theme":"redmond"}', 3, 2);

-- --------------------------------------------------------

--
-- Table structure for table `tbl_users`
--

CREATE TABLE IF NOT EXISTS `tbl_users` (
`id` int(11) NOT NULL,
  `username` varchar(20) NOT NULL,
  `password` varchar(128) NOT NULL,
  `email` varchar(128) NOT NULL,
  `activkey` varchar(128) NOT NULL DEFAULT '',
  `createtime` int(10) NOT NULL DEFAULT '0',
  `lastvisit` int(10) NOT NULL DEFAULT '0',
  `superuser` int(1) NOT NULL DEFAULT '0',
  `status` int(1) NOT NULL DEFAULT '0'
) ENGINE=InnoDB  DEFAULT CHARSET=utf8 AUTO_INCREMENT=9 ;

--
-- Dumping data for table `tbl_users`
--

INSERT INTO `tbl_users` (`id`, `username`, `password`, `email`, `activkey`, `createtime`, `lastvisit`, `superuser`, `status`) VALUES
(1, 'admin', '21232f297a57a5a743894a0e4a801fc3', 'webmaster@example.com', '9a24eff8c15a6a141ece27eb6947da0f', 1261146094, 1415770742, 1, 1),
(2, 'demo', 'fe01ce2a7fbac8fafaed7c982a04e229', 'demo@example.com', '099f825543f7850cc038b90aaff39fac', 1261146096, 0, 0, 1),
(7, 'yuli', 'f58a4d09485609a22c50547646e5282b', 'yuliantioenang@gmail.com', '4612463b486a397bb8299538cd9f1688', 1412464248, 1415770753, 0, 1),
(8, 'yulianti', 'f58a4d09485609a22c50547646e5282b', 'yulianti@gmail.com', '800026964f7703ebec75fd8250494ffa', 1412584709, 1412584709, 0, 1);

--
-- Indexes for dumped tables
--

--
-- Indexes for table `data_penelitian`
--
ALTER TABLE `data_penelitian`
 ADD PRIMARY KEY (`id`);

--
-- Indexes for table `metadata_penelitian`
--
ALTER TABLE `metadata_penelitian`
 ADD PRIMARY KEY (`id`), ADD KEY `id` (`id`);

--
-- Indexes for table `metadata_relasi`
--
ALTER TABLE `metadata_relasi`
 ADD PRIMARY KEY (`id`);

--
-- Indexes for table `relasi`
--
ALTER TABLE `relasi`
 ADD PRIMARY KEY (`id`);

--
-- Indexes for table `saved_map`
--
ALTER TABLE `saved_map`
 ADD PRIMARY KEY (`id`);

--
-- Indexes for table `tbl_profiles`
--
ALTER TABLE `tbl_profiles`
 ADD PRIMARY KEY (`user_id`);

--
-- Indexes for table `tbl_profiles_fields`
--
ALTER TABLE `tbl_profiles_fields`
 ADD PRIMARY KEY (`id`), ADD KEY `varname` (`varname`,`widget`,`visible`);

--
-- Indexes for table `tbl_users`
--
ALTER TABLE `tbl_users`
 ADD PRIMARY KEY (`id`), ADD UNIQUE KEY `username` (`username`), ADD UNIQUE KEY `email` (`email`), ADD KEY `status` (`status`), ADD KEY `superuser` (`superuser`);

--
-- AUTO_INCREMENT for dumped tables
--

--
-- AUTO_INCREMENT for table `data_penelitian`
--
ALTER TABLE `data_penelitian`
MODIFY `id` int(11) NOT NULL AUTO_INCREMENT,AUTO_INCREMENT=176;
--
-- AUTO_INCREMENT for table `metadata_penelitian`
--
ALTER TABLE `metadata_penelitian`
MODIFY `id` int(11) NOT NULL AUTO_INCREMENT,AUTO_INCREMENT=20;
--
-- AUTO_INCREMENT for table `metadata_relasi`
--
ALTER TABLE `metadata_relasi`
MODIFY `id` int(11) NOT NULL AUTO_INCREMENT,AUTO_INCREMENT=3;
--
-- AUTO_INCREMENT for table `relasi`
--
ALTER TABLE `relasi`
MODIFY `id` int(11) NOT NULL AUTO_INCREMENT,AUTO_INCREMENT=14;
--
-- AUTO_INCREMENT for table `saved_map`
--
ALTER TABLE `saved_map`
MODIFY `id` int(11) NOT NULL AUTO_INCREMENT,AUTO_INCREMENT=19;
--
-- AUTO_INCREMENT for table `tbl_profiles_fields`
--
ALTER TABLE `tbl_profiles_fields`
MODIFY `id` int(10) NOT NULL AUTO_INCREMENT,AUTO_INCREMENT=4;
--
-- AUTO_INCREMENT for table `tbl_users`
--
ALTER TABLE `tbl_users`
MODIFY `id` int(11) NOT NULL AUTO_INCREMENT,AUTO_INCREMENT=9;
/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
